{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LR-Lab.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOI6iOZNfQl1OIayUoHGqgt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeb2Binfang/ECE-GY-9163-ML-cyber/blob/main/Lab/LR_Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7_pq0FUN8Qg"
      },
      "source": [
        "\n",
        "#Logistic regression review\n",
        "\n",
        "## Intro to LR\n",
        "\n",
        "LR 是很重要的模型，教授在课上也给我们认真地讲过了，这里就把过程以及推论都给详细地写一下\n",
        "\n",
        "LR是一个很经典的二分类的分类方法，在工业界中，LR也是经常使用的，非常重要，面试也是经常考的，所以我们要去了解怎么推导LR，了解LR的objective function，了解在LR模型下怎么避免overfitting，LR的框架。LR一般都可以作为baseline\n",
        "\n",
        "在很多场景下都是可以用LR的，比如说\n",
        "* 贷款违约情况(会/不会)\n",
        "* 广告点击问题，也就是CTR prediction\n",
        "* 商品推荐\n",
        "* 情感分析\n",
        "* 疾病诊断\n",
        "* 等等\n",
        "\n",
        "我们来看一个例子，是一个classification problem，这是一个二分类的问题\n",
        "\n",
        "现在有这么一个数据\n",
        "\n",
        "| Age      | Salary | Education      | If Breach of contract |\n",
        "| :----: | :----: | :----: | :----: |\n",
        "| 20      | 4000       | undergrad      | Yes     |\n",
        "| 25   | 5000        | college      | No      |\n",
        "| 21      | 6000       | undergrad     | No      |\n",
        "| 25   | 5000       | college     | Yes      |\n",
        "| 28      | 8000      | undergrad      | No       |\n",
        "| 27  | 7000        | undergrad      | ?      |\n",
        "\n",
        "学习输入到输出的映射 $f:X→ Y$\n",
        "\n",
        "$X$就是我们的属入\n",
        "\n",
        "$Y$就是我们的输出\n",
        "\n",
        "这里呢，其实我们也就是想求这么一个概率，也就是$P(Y|X)$。\n",
        "\n",
        "那我们现在思考一个问题，就是我们可不可以用线性回归来表示$P(Y|X)=w^Tx+b$? 为什么？\n",
        "\n",
        "当然是不可以了\n",
        "* 因为$P(Y|X)$是条件概率，它的范围是[0,1]\n",
        "* 其次就是所有类的概率加起来是1，也就是$∑_y P(Y|X)=1$.\n",
        "* 而线性回归的范围是(-∞,+∞)\n",
        "\n",
        "那我们可不可以把线性回归的$w^Tx+b$给映射到[0,1]呢？于是我们就想到了logistic function，也就是\n",
        "$$\n",
        "y=\\frac{1}{1+e^{-z}}, z=w^Tx+b\n",
        "$$\n",
        "它的图像如下图所示\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/68700549/147372757-b5acd7b2-7096-4401-b178-bc4bd07e9ce1.png\" width=\"300\" height=\"200\" />\n",
        "\n",
        "于是，我们LR model的decision function就是\n",
        "$$\n",
        "P(Y|X)=y=\\frac{1}{1+e^{-z}}, z=w^Tx+b\n",
        "$$\n",
        "\n",
        "我们现在回到开头提的classification problem，假设模型已经训练好，那么我们可以得到\n",
        "$$\n",
        "\\begin{gathered}\n",
        "P(Y=\\text{Yes}|(27,7000, \\text{undergrad}))=\\frac{1}{1+e^{-[\\begin{bmatrix}w_1,w_2,w_3\\end{bmatrix}\\begin{bmatrix}27\\\\7000\\\\ \\text{undergrad}  \\end{bmatrix}+b]}}\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "就是这样的一种形式，然后我们看概率，设置threshold，然后就可以知道是Yes还是No了。\n",
        "\n",
        "对于二分类问题，我们有\n",
        "$$\n",
        "\\begin{gathered}\n",
        "P(y=1|x,w)=\\frac{1}{1+e^{-w^Tx+b}}\\\\P(y=0|x,w)=1-P(y=1|x,w)=\\frac{e^{-w^Tx+b}}{1+e^{-w^Tx+b}}\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "上述的两个式子是可以合并成\n",
        "$$\n",
        "p(y|x,w)=P(y=1|x,w)^y[1-P(y=1|x,w)]^{1-y}\n",
        "$$\n",
        "\n",
        "这里我们思考一个问题，就是LR是线性分类器还是非线性分类器？\n",
        "\n",
        "答案就是LR是线性分类器。怎么看呢？我们就要去看其决策边界，decision boundary\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/68700549/147388134-83139c33-1aca-441d-813b-aa67ea01a843.png\" width=\"427\" height=\"159\" />\n",
        "\n",
        "那么怎么看LR的decision boundary呢？这里呢，有一个假设，假设是二分类的情况，如果是线性分类器的话，就是落在每一边的概率是一样的。于是，对LR来说，就有\n",
        "$$\n",
        "\\frac{P(y=1|x,w)}{P(y=0|x,w)}=1\n",
        "$$\n",
        "\n",
        "于是，我们可以得到$e^{-(w^Tx+b)}=1$，两边加ln，可以得到$lne^{-(w^Tx+b)}=ln1$. 于是可以得到$-(w^Tx+b)=0$, 也就是$w^Tx+b=0$,这也就是LR的decision boundary了，所以，LR的decision boundary是线性的，也就说明了LR是linear classifier。\n",
        "\n",
        "\n",
        "## LR Objective function\n",
        "\n",
        "就是我们要去求解$w,b$. LR要优化的函数是最大似然，maximum likelihood。也就是要最大化见到样本的概率。\n",
        "\n",
        "假设我们拥有数据集$D=\\{(x_i,y_i)\\}_{i=1}^n,x_i∈ 𝙍^d, y_i∈ \\{0,1\\}$.\n",
        "\n",
        "而且，我们已经定义了\n",
        "$$\n",
        "p(y|x,w)=P(y=1|x,w)^y[1-P(y=1|x,w)]^{1-y}\n",
        "$$\n",
        "\n",
        "我们需要最大化目标函数， MLE=maximum likelihood estimation\n",
        "$$\n",
        "\\hat{w}_{MLE},\\hat{b}_{MLE}=argmax_{w，b} ∏_{i=1}^n p(y_i|x_i,w,b)\n",
        "$$\n",
        "\n",
        "所以，在LR中，其实我们是需要考虑所有的样本数据的，我们要去寻找$w,b$使得$∏_{i=1}^n p(y_i|x_i,w,b)$最大化。\n",
        "\n",
        "接下来我们来看看怎么最大化目标函数,一般来说，我们看到连乘，$∏$,我们就要考虑加上log，log又是单调递增函数，而且会使得连乘运算变得简单\n",
        "$$\n",
        "\\begin{gathered}\n",
        "\\hat{w}_{MLE},\\hat{b}_{MLE}=argmax_{w，b} ∏_{i=1}^n p(y_i|x_i,w,b)\\\\=argmax_{w,b}log(∏_{i=1}^n p(y_i|x_i,w,b))\\\\=argmax_{w,b}\\sum_{i=1}^n log P(y_i|x_i,w,b)\\\\=argmin_{w,b}-\\sum_{i=1}^n log P(y_i|x_i,w,b)\\\\=argmin_{w,b}-\\sum_{i=1}^n log[P(y_i=1|x,w)^y[1-P(y_i=1|x,w)]^{1-y_i}] \\\\=argmin_{w,b}-\\sum_{i=1}^n y_ilog P(y_i=1|x_i, w,b)+(1-y_i)log[1-P(y_i=1|x_i,w,b)]\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "这也就是LR 的objective function，一定要记住，也要去理解这是怎么来的\n",
        "\n",
        "## Gradient descent\n",
        "\n",
        "我们已经知道了LR 的objective function，接下来就是要知道怎么去优化。在了解怎么优化之前，我们先了解什么是凸函数，上课讲过的，不难理解。我们现在要知道得到的是全局最优解还是局部最优解\n",
        "<img src=\"https://user-images.githubusercontent.com/68700549/147388993-10fbc081-c530-4340-8e1f-63cacd520400.png\" width=\"506\" height=\"179\" />\n",
        "\n",
        "典型的非凸就是神经网络\n",
        "\n",
        "判断一个函数是否是凸函数，就需要看这个函数是否满足\n",
        "$$\n",
        "f(λx_1+(1-λ)x_2) ≤ λf(x_1)+(1-λ)f(x_2)\n",
        "$$\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/68700549/147389044-b44655d7-1189-4e01-adfe-f09710bf0e20.png\" width=\"506\" height=\"179\" />\n",
        "\n",
        "常见的convex function有\n",
        "* 线性函数: $f(x)=a^Tx+b$\n",
        "* 二次函数: $f(x)=x^TQx+a^Tx+b$, $Q∈ S_+^n$\n",
        "* least square: $f(x)=||Ax-b||_2^2$\n",
        "* p-norm: $f(x)=(∑_{i=1}^n|x_i|^p)^{\\frac{1}{p}}$, $p ≥ 1$\n",
        "* log-sum-exp function: $f(x)=log(∑_{i=1}^n e^{x_i}),x\\in R^n$\n",
        "\n",
        "\n",
        "GD的过程很简单，不难，它的伪代码如下\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# GD pseudocode\n",
        "Init w_1\n",
        "for t=1,2...\n",
        "  w^{t+1}=w^t-η▽f(w^t)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "logistic regression本身也是凸函数的\n",
        "\n",
        "\n",
        "接下来我们要看看如何用GD来求解LR model的最优解，我们一直LR的objective function为\n",
        "$$\n",
        "argmin_{w,b}-\\sum_{i=1}^n y_ilog P(y_i=1|x_i, w,b)+(1-y_i)log[1-P(y_i=1|x_i,w,b)]\n",
        "$$\n",
        "\n",
        "接下来我们就要对其进行求导，我们令$L(w,b)=-\\sum_{i=1}^n y_ilog P(y_i=1|x_i, w,b)+(1-y_i)log[1-P(y_i=1|x_i,w,b)$\n",
        "\n",
        "于是，我们对其进行求导. 我们知道sigmoid函数的求导就是 $σ'(x)=σ(x)[1-σ(x)]$. 我们先对$w$进行求导\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "\\frac{∂ L(w,b)}{\\partial w}=-∑_{i=1}^n y_i \\frac{σ (w^Tx_i+b)[1-σ (w^Tx_i+b)]}{σ (w^Tx_i+b)}x_i+(1-y)\\frac{σ (w^Tx_i+b)[1-σ (w^Tx_i+b)]}{1-σ (w^Tx_i+b)}x_i\\\\ = -∑_{i=1}^n y_i (1-σ (w^Tx_i+b))x_i + (y-1)σ (w^Tx_i+b)x_i\\\\ = -∑_{i=1}^n [y_i -σ (w^Tx_i+b)]x_i\\\\ = ∑_{i=1}^n [σ (w^Tx_i+b)-y_i]x_i\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "我们可以看到LR在做GD的时候，总是在预测值和真实值中做比较，也就是$σ (w^Tx+b)-y_i$\n",
        "\n",
        "现在我们再来对$b$进行求导\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "\\frac{∂ L(w,b)}{\\partial b}=-∑_{i=1}^n y_i \\frac{σ (w^Tx_i+b)[1-σ (w^Tx_i+b)]}{σ (w^Tx_i+b)}+(1-y)\\frac{σ (w^Tx_i+b)[1-σ (w^Tx_i+b)]}{1-σ (w^Tx_i+b)}\\\\ = -∑_{i=1}^n y_i (1-σ (w^Tx_i+b)) + (y-1)σ (w^Tx_i+b)\\\\ = -∑_{i=1}^n [y_i -σ (w^Tx_i+b)]\\\\ = ∑_{i=1}^n [σ (w^Tx_i+b)-y_i]\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "那么，GD在什么时候停止iteration呢？有以下几种方法\n",
        "* $|L^t(w,b)-L^{t+1}(w,b)|<ϵ$\n",
        "* $|w^t-w^{t+1}|<ϵ$\n",
        "* check validation set accuracy, early stopping\n",
        "* fixed itertions\n",
        "\n",
        "我们思考一下如果用GD，会有什么问题？我们看到我们更新参数时，总是有$∑_{i=1}^n$, 也就是说总是要循环所有的样本，在大数据的情况下，效率会慢。于是我们需要考虑新的方法，也就是stochastic GD。 SGD也就是每次随机更新一个样本，这样也能够有效地避免saddle point，也减少了计算量. GD和SGD的pseudocode如下图所示。\n",
        "\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/68700549/147393452-1f01bacf-e1d2-43d3-b35d-b51dc2cead43.jpg\" width=\"506\" height=\"433\" />\n",
        "\n",
        "我们可以看到，SGD是更新非常频繁的，每看到一个样本，都要去计算一下其误差。GD的话是$∑$, 也就是假设有$n$个样本，我要把这$n$个样本都给$∑$一下，才会更新其参数。SGD的话是每个样本，就单个样本进行更新，这样计算效率就会加快，但是也有缺点，就是数据中是存在噪声的，这样会导致更新不准确，于是需要使用较小的learning rate，而GD是可以使用较大的learning rate的。 SGD是经常使用的，所以我们一定要知道是怎么work的。\n",
        "\n",
        "SGD每次更新一个样本也是较慢的，于是就有了mini-batch SGD，也就是每次随机地从$n$个sample中取$m$个数据，一般取32,64,128。这样我们就可以用来更新$w,b$了， pseudocode如下图所示\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/68700549/147393630-e2ee41b4-6115-4cbb-b7cc-a9bbfcbecc03.jpg\" width=\"506\" height=\"192\" />\n",
        "\n",
        "Mini-batch的优势在于不用考虑所有的样本，比起SGD也相对比较稳定，因为不是考虑一个样本，而是同时考虑多个样本\n",
        "\n",
        "## Convergence analysis of GD\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_VSfOj8N5ww"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}