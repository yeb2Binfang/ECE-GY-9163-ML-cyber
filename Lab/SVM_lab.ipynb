{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVM_lab.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOWIjfAMRoS6q1VeYNQb6fU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeb2Binfang/ECE-GY-9163-ML-cyber/blob/main/Lab/SVM_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZex4Ns9zMac"
      },
      "source": [
        "目录"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZiCfDOE3jsb"
      },
      "source": [
        "# Basic knowledge of SVM\n",
        "\n",
        "在ML cyber这门课上，教授给我们review了SVM。我们需要了解SVM是怎么work的，也需要知道其数学过程。在我们了解其过程之后，我们会做一个小lab来加深对其的理解。\n",
        "\n",
        "In Machine Learning Cyber class, we reviewed SVM. We need to know how does SVM work and understand its mathematical process also. After review them, we will do a small project to have a deep understanding \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq9ky35Fa4jR"
      },
      "source": [
        "## The intuitive understanding of SVM\n",
        "\n",
        "Now, we have two classes that can be linearly separated. Below figure shows the case we are assuming.\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/68700549/134811043-62c61f8e-38fd-492e-bff7-07e7d2a2c77b.png\" alt=\"WeChat Screenshot_20210926095925\" style=\"zoom:10%;\" />\n",
        "\n",
        "现在呢，我们可以看到有两个类，可以被线性分开，图中我们也可以看到有两条线都可以将其分开，但是哪一条更好呢？\n",
        "\n",
        "Now, we can that there are two lines can linearly separated the classess. But which one is better? \n",
        "\n",
        "\n",
        "![WeChat Screenshot_20210926100557](https://user-images.githubusercontent.com/68700549/134811213-072c5509-c815-489c-a52d-7efe36e72470.png)\n",
        "\n",
        "于是就有了SVM，我们希望能找到一条线，就是找到两个类的边界，然后我们找到的线可以使得到这两条线的距离相等。就如SVM的图所示.\n",
        "\n",
        "SVM, support vector machine, will help find a line that can linearly separated them well. The SVM will find the boundary of both classes and the line will be posited between them. We hope the distance between both classes' boundary points can be equal. Look at the graph above.\n",
        "\n",
        "假设能找到线，我们就希望能够最大化这个距离，让两个classes尽可能地分开。如果有新数据进来，那么我们找到的线也是能够尽可能地正确分类新数据。\n",
        "\n",
        "We try to maximize the distance of the margin to separate two classes as much as possible. If new data comes in, the line has much higher probability to classifier it well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hQUIt31e2eG"
      },
      "source": [
        "## The mathematic of SVM\n",
        "\n",
        "现在我们来算一下穿过support vector (两个classes的边界点)的两条线之间的距离是多少，从图中，我们可以看到有两条线，一个是$wx+b=1$, 另一条是$wx+b=-1$. 这里先解释下为什么是1和-1，即使是其他的数，我们也是可以变换到1和-1来的，就是做乘除法而已。\n",
        "\n",
        "Now, we are calculating the distance between two lines that pass throught the support vecotrs (show in the graph). We can see there are two dashed line, one is $wx+b=1$, another one is $wx+b=-1$. The reason why it is 1 and -1. It actually does not matter. We can transform it to 1 and -1 even though they are not. Just do some multiplication and division. Using 1 and -1 is much easier. \n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/68700549/134814170-94381088-5f22-4ba3-910b-92a523d453d8.png\" alt=\"WeChat Screenshot_20210926112656\" style=\"zoom:50%;\" />\n",
        "\n",
        "这两条线的距离其实就是$d_1+d_2$​​​. 我们需要知道点到线的距离的计算公式，如下\n",
        "\n",
        "The distance between these two lines is $d_1+d_2$. The way to calculate the distance from a point to a line is shown below.​\n",
        "$$\n",
        "r = \\frac{|wx+b|}{||w||_2}\n",
        "$$\n",
        "\n",
        "现在，我们要计算$d_1+d_2$​, 也就是要看support vector，那么也就是\n",
        "\n",
        "Now, we need to calculate the $d_1+d_2$. So, we need to look at the support vectors. Then we get the distance.​\n",
        "\n",
        "$$\n",
        "\\gamma = d_1+d_2=\\frac{|wx_1+b|+|wx_2+b|}{||w||_2}=\\frac{2}{||w||_2}\n",
        "$$\n",
        "\n",
        "然后，现在我们想要最大化这个距离.于是我们就要做一个约束优化. 上面的$\\frac{2}{||w||_2}$可以写成下面的形式. $y_i(w^Tx_i+b)\\ge 1$, 也就是分类正确的意思\n",
        "\n",
        "We want to maximize the distance. We can do a constraint optimization. $y_i(w^Tx_i+b)\\ge 1$ means classify points correctly.\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "min_{w,b}\\frac{1}{2}||w||^2\\\\\n",
        "s.t.  \\space y_i(w^Tx_i+b)\\ge 1, i=1,\\cdots, n\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "遇到约束优化，我们就可以进行拉格朗日化，但前提要变成对偶条件。也就是primal dual，对于原优化问题\n",
        "\n",
        "when we have constraint optimization, we can think about Lagrange multipliers. We should make the optimization become primal dual problem. We should make things like below\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "\\mathop{min}\\limits_{w} f(w)\\\\\n",
        "s.t. \\space\\space \n",
        "\\begin{matrix}\n",
        "g_i(w)\\le 0\\\\\n",
        "h_i(w)\\le 0\n",
        "\\end{matrix}\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "这种形式下，它的拉格朗日函数如下\n",
        "\n",
        "In this format, its Lagrange function is below\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(w,\\alpha,\\beta)=f(w)+\\sum_{i=1}^k \\alpha_i g_i(w)+\\sum_{i=1}^l \\beta_i h_i(w)\n",
        "$$\n",
        "\n",
        "此时，问题就变成了$\\mathop{max}\\limits_{\\alpha,\\beta:\\alpha\\ge0}\\mathcal{L}(w,\\alpha,\\beta)$​\n",
        "\n",
        "Now, the question becomes $\\mathop{max}\\limits_{\\alpha,\\beta:\\alpha\\ge0}\\mathcal{L}(w,\\alpha,\\beta)$\n",
        "\n",
        "同理，对于我们的SVM，也是相同的做法，我们先把原始优化变成primal dual的形式，也就是\n",
        "\n",
        "So, for SVM, we do the same thing. We need to make it prime dual first. It is like below.\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "min_{w,b}\\frac{1}{2}||w||^2\\\\\n",
        "s.t.  \\space 1-y_i(w^Tx_i+b)\\le 0, i=1,\\cdots, n\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "于是，它的拉格朗日函数变成了\n",
        "\n",
        "And its Lagrange function is below. \n",
        "\n",
        "$$\n",
        "\\mathcal{L}(w,b,\\alpha)=\\frac{1}{2}||w||^2 + \\sum_{i=1}^n \\alpha_i (1-y_i(w^Tx_i+b))\n",
        "$$\n",
        "\n",
        "此时，我们要对$w,b$​进行求导\n",
        "\n",
        "Then, we can get the partial derivative of $w,b$.\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "\\triangledown_w\\mathcal{L}(w,b,\\alpha)=w-\\sum_{i=1}^n \\alpha_i y_i x_i=0\\\\\n",
        "\\triangledown_b\\mathcal{L}(w,b,\\alpha)=-\\sum_{i=1}^n \\alpha_i y_i=0\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "于是，我们可以得到 $w=\\sum_{i=1}^n \\alpha_i y_i x_i$​ and $\\sum_{i=1}^n \\alpha_i y_i=0$​​. \n",
        "\n",
        "Now, we can get  $w=\\sum_{i=1}^n \\alpha_i y_i x_i$ and $\\sum_{i=1}^n \\alpha_i y_i=0$. \n",
        "\n",
        "然后我们再把$w$​的值给代回到拉格朗日函数中，可以得到\n",
        "\n",
        "And we put $w$ into the Lagrange function\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "\\mathcal{L}(w,b,\\alpha)=\\frac{1}{2}||\\sum_{i=1}^n \\alpha_i y_i x_i||^2+\\sum_{i=1}^n \\alpha_i(1-y_i((\\sum_{j=1}^n \\alpha_j y_j x_j)^Tx_i +b))\\\\\n",
        "=\\frac{1}{2}(\\sum_{i=1}^n \\alpha_i y_i x_i)^T (\\sum_{j=1}^n \\alpha_j y_j x_j)+\\sum_{i=1}^n \\alpha_i -\\sum_{i=1}^n \\alpha_i y_i(\\sum_{j=1}^n \\alpha_j y_j x_j)^Tx_i - \\sum_{i=1}^n \\alpha_i y_ib\\\\\n",
        "=\\frac{1}{2}\\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j x_i^T x_j+\\sum_{i=1}^n \\alpha_i -\\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j x_i^T x_j\\\\\n",
        "=\\sum_{i=1}^n \\alpha_i-\\frac{1}{2}\\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j x_i^T x_j\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "现在，我们函数中已经没有了$w,b$, 只剩下了$\\alpha$​. 所以，我们构造的对偶问题就是\n",
        "\n",
        "Now, in the function, we do not have $w,b$ and only $\\alpha$ left, so the dual problem we constructed is shown below\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "max_{\\alpha}= \\sum_{i=1}^n \\alpha_i-\\frac{1}{2}\\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j x_i^T x_j\\\\\n",
        "s.t. \\space \\space\n",
        "\\begin{matrix}\\alpha_i \\ge0\\\\\n",
        "\\sum_{i=1}^n \\alpha_i y_i=0\n",
        "\\end{matrix}\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "SVM的决策式子，也就是SVM的模型，就可以写成, 这里呢，每一个$x$，都会有一个对应的$\\alpha$​.\n",
        "\n",
        "SVM decision formula is shown below. We can know for every $x$, there is a corresponding $\\alpha$\n",
        "\n",
        "$$\n",
        "f(x)=w^Tx+b=\\sum_{i=1}^n \\alpha_i y_i x_i^T x +b\n",
        "$$\n",
        "\n",
        "这里，我们会有一个疑问，每次测试一个新样本，就需要跟每一个数据进行转置乘积并求和，哇，假设有一个亿的数据，那岂不是要有很大的空间来存储。看到这里，我们必须要清楚，我们的式子是有不等式约束的，也就是要满足KKT条件.\n",
        "\n",
        "Here, we may have a question. When we test a new test data, then we need to transpose every $x$​​ and get the cumulative sum. If we have a billion data, then we need a lot of space to store it. But, we need to know, our formula we got lastly satisfies the kKT conditions which is shown below\n",
        "\n",
        "$$\n",
        "\\begin{cases}\n",
        "\\alpha_i\\ge0 \\\\\n",
        "y_if(x_i)-1\\ge0\\\\\n",
        "\\alpha_i(y_if(x_i)-1)=0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "所以，我们是不用担心这么多数据的，根据KKT条件的第三行，可以得知非support vector的$\\alpha=0$. 只有support vector，$y_if(x_i)-1=0$​. 所以，这里就有一个SVM的重要性质，就是训练完成后，大部分的训练样本都不需要保留，最终模型只跟support vector相关。\n",
        "\n",
        "So, we do need to worry about it. According to the third line of KKT conditions, we can know that the $\\alpha$ value of non support vectors are all equal to 0. Therefore, there is a very important property of SVM that the final model only associates with support vectors and most of training data does not need to be stored.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkB8jGtZazTs"
      },
      "source": [
        "## SVM with slack variable\n",
        "\n",
        "我们的数据集中，是会有噪音的，造成很难线性完全可分。就像下面的这个例子，会相对来说比较难划分，所以我们可以用soft-margin.\n",
        "\n",
        "It is very normal that we have noise in the data. It will be very hard to be linearly separated. Let's look up the picture below, it will be hard to separate, so that we have to use soft-margin.\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/68700549/123352808-1c9a0580-d52e-11eb-8307-70dc6dc28303.png\" alt=\"WeChat Screenshot_20210624205225\" style=\"zoom:50%;\" />\n",
        "\n",
        "soft-margin的意思就是在满足maximum margin的同时，允许一些可出错的情况，但是要是这个可出错的情况尽可能少 。我们加一个slack variable，优化公式就变为\n",
        "\n",
        "soft-margin means when the line satisties the maximum margin, we allow the classifier to make some mistakes. But we try to avoid this mistakes as much as possible. We add a slack variable, so the optimize formula is:\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "\\mathop{min}\\limits_{w,b}\\frac{1}{2}||w||^2+C\\sum_{i=1}^m \\xi_i\\\\\n",
        "s.t. \\space \n",
        "\\begin{matrix}y_i(w^Tx_i+b)\\ge 1-\\xi_i\\\\\n",
        "\\xi_i\\ge0,i=1,2,...,m\n",
        "\\end{matrix}\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "加入 $\\xi$ 之后，我们就可以允错一些数据了，原来是要大于等于1，现在值变小了，大于等于$1-\\xi$就好了，所以会允错。每一个样本都会对应一个$\\xi$.因为我们还是要去求$w,b$,同时也是要优化$\\xi_i$的，所以要先转化成dual问题，第一步就是Lagrange 化\n",
        "\n",
        "After we added variable $\\xi$, it means we allow the classifier to make some mistakes. Originally, the formula only arrows >=1 and now, it becomes $1-\\xi_i$. So, the value becomes smaller. Every point will have a corresponding $\\xi_i$. We still need to get $w,b$ and at the same time, we need to optimize the $\\xi_i$ too. So, we can have it Lagrange format, that is\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(w,b,\\xi,\\alpha,\\mu)=\\frac{1}{2}||w||^2+C\\sum_{i=1}^m \\xi_i-\\sum_{i=1}^m \\alpha_i(y_i(w^Tx_i+b)+\\xi_i-1)-\\sum_{i=1}^m \\mu_i\\xi_i\n",
        "$$\n",
        "\n",
        "于是，我们需要对$w,b,\\xi$​进行求偏导得0，于是，我们就可以得到\n",
        "\n",
        "Now, we can get the partial derivative of $w,b,\\xi$​ and let them become 0\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "\\triangledown_w \\mathcal{L}(w,b,\\xi,\\alpha,\\mu)=w-\\sum_{i=1}^m \\alpha_i y_ix_i=0\\\\\n",
        "\\triangledown_b \\mathcal{L}(w,b,\\xi,\\alpha,\\mu)=\\sum_{i=1}^m \\alpha_iy_i=0\\\\\n",
        "\\triangledown_{\\xi_i} \\mathcal{L}(w,b,\\xi,\\alpha,\\mu)=C-\\alpha_i-\\mu_i=0\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "代入之后，可以得到\n",
        "\n",
        "we put what we got into the formula\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "\\mathcal{L}(w,b,\\xi,\\alpha,\\mu)=\\frac{1}{2}\\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i\\alpha_jy_iy_jx_i^T x_j-\\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i\\alpha_jy_iy_jx_i^T x_j+C\\sum_{i=1}^m \\xi_i -\\sum_{i=1}^m \\alpha_i \\xi_i-\\sum_{i=1}^m \\mu_i \\xi_i+\\sum_{i=1}^m \\alpha_i\\\\\n",
        "=\\sum_{i=1}^m \\alpha_i-\\frac{1}{2}\\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i\\alpha_jy_iy_jx_i^T x_j+\\sum_{i=1}^m \\xi_i (C-\\alpha_i-\\mu_i)\\\\\n",
        "=\\sum_{i=1}^m \\alpha_i-\\frac{1}{2}\\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i\\alpha_jy_iy_jx_i^T x_j\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "我们可以得到其对偶问题\n",
        "\n",
        "We can get the dual problem of SVM with slack variable\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "\\mathop{max}\\limits_{\\alpha} \\space \\sum_{i=1}^m a_i -\\frac{1}{2}\\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j x_i^Tx_j \\\\\n",
        "s.t. \\space\\space \n",
        "\\begin{matrix}\\sum_{i=1}^m a_i y_i=0 \\\\\n",
        "0\\le\\alpha_i\\le C,\\space\\space i=1,2,...,m\n",
        "\\end{matrix}\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "对于上面的公式，我们要知道$y_iy_j$,如果两个训练数据点属于同一类别会使值增加，否则减小。$x_i^Tx_j$是衡量两个数据点的相似性。再看$\\sum_{i=1}^m a_i y_i=0$. 不同数据点的$\\alpha$值会不一样，但是，对于不同类别(比如说，+1,-1,两种类别)，则权重一致，也就是属于+1样本的权重和，属于-1样本的权重和，这两个的绝对值会相等，因为要和为0.\n",
        "\n",
        "Here, we explain the above formula. $y_iy_j$ means if two training data point are the same class, it will increase the value, decrease otherwise. $x_i^Tx_j$ will measure the similarity of two data points. Let's see$\\sum_{i=1}^m a_i y_i=0$, different data points will have different $\\alpha_i$.\n",
        "\n",
        "对于soft-margin的KKT条件是\n",
        "The KKT conditions of soft-margin are\n",
        "\n",
        "$$\n",
        "\\begin{cases}\n",
        "\\alpha_i\\ge0,\\mu_i\\ge0 \\\\\n",
        "y_if(x_i)-1+\\xi_i\\ge0\\\\\n",
        "\\alpha_i(y_if(x_i)-1+\\xi_i)=0\\\\\n",
        "\\xi_i\\ge0,\\mu_i\\xi_i=0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "同样，soft-margin SVM也是只跟support vectors相关。加上slack variables之后，界限被放宽松了，support vector就不单单是在线上的，还包括一些允错范围内的数据点，也算是support vector.\n",
        "\n",
        "Same here, soft-margin SVM only associate with support vectors. After we added slack variable, it will relax the boundary. At this time, support vector is not only lying on teh line but also in the range of \"mistake zone\". \n",
        "\n",
        "这里我们要讨论一下这个variable C\n",
        "\n",
        "Here, we need to discuss the effect of variable C\n",
        "\n",
        "Effects of C: C值越大，margin越小.C 越高，容易过拟合。C 越小，容易欠拟合。\n",
        "\n",
        "Effects of C: larger C, smaller margin, easy to overfit. smaller C larger margin, easy to underfit.\n",
        "\n",
        "我们来看下下面这张图，可以看到C值越大，margin越小，C值越小，margin越大\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/68700549/123664644-83a50c00-d805-11eb-9a1a-e7cd4805bd62.png\" alt=\"WeChat Screenshot_20210628113916\" style=\"zoom:50%;\" />\n",
        "\n",
        "参数C越大，支持向量的数量越少.当 C 逐渐增大的时候，判定边界也越来越复杂，过拟合的风险越来越大，同时，我们也发现支持向量（白色边框的点）的数量越来越少。这是因为当 C 增大时，对于误差的惩罚增大，判定边界趋向于将每一个点都正确地分类，导致支持向量机的margin越来越窄，从而使得能成为支持向量的点的数量越来越少。\n",
        "\n",
        "The larger C, the less support vectors. When we increase C value, the decision boundary becomes more complexity,which will result in overfitting. Meanwhile, we notice that the number of support vectors decrease because larger C will increase the penality of error. The decision boundary tries to classify every point correctly. The smaller margin, the less support vectors.\n",
        "\n",
        "我们看下面这张图，C越大，它的决策边界就越复杂，容易造成overfitting\n",
        "\n",
        "we can look at the below picture, larger C has a more complex decision boundary \n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/68700549/124841610-a51b9b80-df5b-11eb-82d3-e10b747eebb5.jpg\" alt=\"v2-285f04e1e50ed9a840e4e7a0f19032c5_r\" style=\"zoom:67%;\" />\n",
        "\n",
        "对参数C的总结\n",
        "* smaller C\n",
        "\n",
        "    *  will have large margin\n",
        "    *  allow more violations of margin\n",
        "    *  more support vectors\n",
        "    *  reduce variance\n",
        "\n",
        "\n",
        "* larger C\n",
        "    *  small margin\n",
        "    *  reduce violations and fewer support vectors\n",
        "    *  highly fit to the data. Low bias, and higher variance\n",
        "    *  more chance to overfit\n",
        "\n",
        "\n",
        "接下来讲解一下Hinge loss，SVM里面用的就是Hinge loss。先来看看我们通常用的loss有哪些\n",
        "\n",
        "Later, we will explain the hinge loss which is used in SVM. Let's take a look the loss functions we often use.\n",
        "\n",
        "常用loss function\n",
        "*  Zero-one loss\n",
        "*  Hinge loss: $l_{hinge}(z)=max(0,1-z)$\n",
        "*  Exponential loss: $l_{exp}(z)=exp(-z)$\n",
        "*  Logistic loss: $l_{log}(z)=log(1+exp(-z))$\n",
        "\n",
        "然后我们再看看下每个loss的图像是怎么样的\n",
        "\n",
        "And we need to know the graph of each loss functions above\n",
        "\n",
        "<img width=\"792\" alt=\"lossfunction\" src=\"https://user-images.githubusercontent.com/68700549/123355139-0b9fc300-d533-11eb-8d59-56fbdc332985.png\" style=\"zoom:50%;\" >\n",
        "\n",
        "我们这里从Hinge loss的角度来看$\\xi$, 也就是我们允错，error，我们的$\\xi_i\\ge 0$.\n",
        "\n",
        "From Hinge loss's view, we may have a good understanding of $\\xi$. We know that $\\xi_i \\ge 0$\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "y_i(w^Tx_i+b)\\ge1-\\xi_i\\\\\n",
        "\\downarrow\\\\\n",
        "\\xi_i\\ge 1-y_i(w^Tx_i+b)\\\\\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "所以，我们可以总结出\n",
        "\n",
        "So, we can conclude ethat \n",
        "\n",
        "$$\n",
        "\\xi_i=max(0,1-y_i(w^Tx_i+b))\n",
        "$$\n",
        "\n",
        "但是SVM这里为什么要用hinge loss呢？\n",
        "\n",
        "But why SVM prefers hinge loss?\n",
        "\n",
        "使用hinge loss后，仍旧保持了稀疏性。我们先看logistic loss，它在后面是不断趋近于0的，但是不等于0，即使SVM分类正确，仍旧有loss，因为不等于0. SVM在1之后，就全部等于0了，也就是分类正确之后，就把那个loss变为0了，这也会有个好处，就是对outlier不敏感，也就是说，我们看logistic，都是在分类正确的前提下，假设分类得好，loss就小，分类得相对差点，loss就大。而Hinge的话就一视同仁，也是在分类正确的前提下，不管你有多好，loss都是0，这也使得SVM的解具有稀疏性. 而且，在假设分类不正确的情况下，因为我们最后是要求导的，从而update这个$w,b$的，我们看logistic，如果分类不正确，越不正确，loss就越大，在gradient的过程中，稍微调一点，变化就很大。而hinge依旧是一视同仁，loss，分得越差，loss越大，但是求导后都是-1.\n",
        "\n",
        "After the SVM used hinge loss, it still can keep the sparsity. Let's take a look at the logistic loss, it will close to 0 but not equal to 0, which means that even if we classify the points correctly, there is still having loss.  But if we use hinge loss, we can see the graph that the loss is equal to 0 after x=1. It means if we classify points correctly, there is no loss any more. For logistic, it will have a small loss if the model classify it well and a large loss otherwise. But Hinge loss is different because it will view all misclassification in a same way. Whatever how well the model classifies, the loss is 0 and because of it, the model has the sparsity. If the model misclassify the points, for logistic, the partial derivative will be very large if the model classifies very bad, for hinge loss, the partial derivative will be same, all of them will be -1. If it is very large, when we update the variable $w,b$, it will update too much which may miss the optimizaiton point.\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/68700549/123664015-f6fa4e00-d804-11eb-91d3-fa7cab702f19.png\" alt=\"WeChat Screenshot_20210628113531\" style=\"zoom:50%;\" />\n",
        "\n",
        "Hinge loss的优点\n",
        "\n",
        "* Convex function，容易优化\n",
        "* 在自变量小于0的部分，梯度较小(-1),对错误分类的penalty较小\n",
        "* 在自变量大于0的部分，值为0.只要对某个数据分类是正确的，即使分类正确的可能性足够高，也不用针对这个数据进一步优化了\n",
        "* 在自变量为0处不可求导，需要分段进行求导\n",
        "* 求解最优化时，只有support vectors是确定分界线，而且support vector的个数是远远小于训练数据的个数的\n",
        "\n",
        "The advantages of hinge loss\n",
        "\n",
        "* it is a convex function, easy to optimize\n",
        "* for misclassification part, the partial derivaive is -1, it will not penalize a lot\n",
        "* It will not have a further optimizaiton if it classifies correctly because the partial derivative is 0.\n",
        "* The partial derivative at point 0 has to separate\n",
        "* When we optimization the model, only support vectors can decide the decision boundary and the number of support vectors is far less than the number of training data.\n",
        "\n",
        "Logistic loss的优势主要在于其输出具有自然的概率意义，就是在给出预测标记的用时也给出了概率，能直接用于multi-classification。但是，不具有稀疏性，且依赖于更多的训练样本，预测开销大\n",
        "\n",
        "The main advantage of hinge loss is that it can give us the classification probability and it can be applied to multi-classification firectly. However, it does not have the sparsity property and it needs to depend on training data. It needs a lot of memory too."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ImzO22NCBOt"
      },
      "source": [
        "## 带kernel的SVM\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3n6qRiuCcVA"
      },
      "source": [
        "## SVM的smo解法\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWfp3ITRCgdj"
      },
      "source": [
        "## 使用SVM多类别分类"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6UyJh2PYjDL"
      },
      "source": [
        "# SVM人脸识别结合cross-validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnpVLVXxYrAG"
      },
      "source": [
        "# 模型评估方法和SVM做人脸识别"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuxb08Nocv7J"
      },
      "source": [
        "# The interview questions of SVM"
      ]
    }
  ]
}