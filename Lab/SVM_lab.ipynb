{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVM_lab.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP02WrFepoC5F4t0vXSvGuy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeb2Binfang/ECE-GY-9163-ML-cyber/blob/main/Lab/SVM_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZiCfDOE3jsb"
      },
      "source": [
        "# Basic knowledge of SVM\n",
        "\n",
        "在ML cyber这门课上，教授给我们review了SVM。我们需要了解SVM是怎么work的，也需要知道其数学过程。在我们了解其过程之后，我们会做一个小lab来加深对其的理解。\n",
        "\n",
        "In Machine Learning Cyber class, we reviewed SVM. We need to know how does SVM work and understand its mathematical process also. After review them, we will do a small project to have a deep understanding \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq9ky35Fa4jR"
      },
      "source": [
        "## The intuitive understanding of SVM\n",
        "\n",
        "Now, we have two classes that can be linearly separated. Below figure shows the case we are assuming.\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/68700549/134811043-62c61f8e-38fd-492e-bff7-07e7d2a2c77b.png\" alt=\"WeChat Screenshot_20210926095925\" style=\"zoom:10%;\" />\n",
        "\n",
        "现在呢，我们可以看到有两个类，可以被线性分开，图中我们也可以看到有两条线都可以将其分开，但是哪一条更好呢？\n",
        "\n",
        "Now, we can that there are two lines can linearly separated the classess. But which one is better? \n",
        "\n",
        "\n",
        "![WeChat Screenshot_20210926100557](https://user-images.githubusercontent.com/68700549/134811213-072c5509-c815-489c-a52d-7efe36e72470.png)\n",
        "\n",
        "于是就有了SVM，我们希望能找到一条线，就是找到两个类的边界，然后我们找到的线可以使得到这两条线的距离相等。就如SVM的图所示.\n",
        "\n",
        "SVM, support vector machine, will help find a line that can linearly separated them well. The SVM will find the boundary of both classes and the line will be posited between them. We hope the distance between both classes' boundary points can be equal. Look at the graph above.\n",
        "\n",
        "假设能找到线，我们就希望能够最大化这个距离，让两个classes尽可能地分开。如果有新数据进来，那么我们找到的线也是能够尽可能地正确分类新数据。\n",
        "\n",
        "We try to maximize the distance of the margin to separate two classes as much as possible. If new data comes in, the line has much higher probability to classifier it well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hQUIt31e2eG"
      },
      "source": [
        "## The mathematic of SVM\n",
        "\n",
        "现在我们来算一下穿过support vector (两个classes的边界点)的两条线之间的距离是多少，从图中，我们可以看到有两条线，一个是$wx+b=1$, 另一条是$wx+b=-1$. 这里先解释下为什么是1和-1，即使是其他的数，我们也是可以变换到1和-1来的，就是做乘除法而已。\n",
        "\n",
        "Now, we are calculating the distance between two lines that pass throught the support vecotrs (show in the graph). We can see there are two dashed line, one is $wx+b=1$, another one is $wx+b=-1$. The reason why it is 1 and -1. It actually does not matter. We can transform it to 1 and -1 even though they are not. Just do some multiplication and division. Using 1 and -1 is much easier. \n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/68700549/134814170-94381088-5f22-4ba3-910b-92a523d453d8.png\" alt=\"WeChat Screenshot_20210926112656\" style=\"zoom:50%;\" />\n",
        "\n",
        "这两条线的距离其实就是$d_1+d_2$​​​. 我们需要知道点到线的距离的计算公式，如下\n",
        "\n",
        "The distance between these two lines is $d_1+d_2$. The way to calculate the distance from a point to a line is shown below.​\n",
        "$$\n",
        "r = \\frac{|wx+b|}{||w||_2}\n",
        "$$\n",
        "\n",
        "现在，我们要计算$d_1+d_2$​, 也就是要看support vector，那么也就是\n",
        "\n",
        "Now, we need to calculate the $d_1+d_2$. So, we need to look at the support vectors. Then we get the distance.​\n",
        "\n",
        "$$\n",
        "\\gamma = d_1+d_2=\\frac{|wx_1+b|+|wx_2+b|}{||w||_2}=\\frac{2}{||w||_2}\n",
        "$$\n",
        "\n",
        "然后，现在我们想要最大化这个距离.于是我们就要做一个约束优化. 上面的$\\frac{2}{||w||_2}$可以写成下面的形式. $y_i(w^Tx_i+b)\\ge 1$, 也就是分类正确的意思\n",
        "\n",
        "We want to maximize the distance. We can do a constraint optimization. $y_i(w^Tx_i+b)\\ge 1$ means classify points correctly.\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "min_{w,b}\\frac{1}{2}||w||^2\\\\\n",
        "s.t.  \\space y_i(w^Tx_i+b)\\ge 1, i=1,\\cdots, n\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "遇到约束优化，我们就可以进行拉格朗日化，但前提要变成对偶条件。也就是primal dual，对于原优化问题\n",
        "\n",
        "when we have constraint optimization, we can think about Lagrange multipliers. We should make the optimization become primal dual problem. We should make things like below\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "\\mathop{min}\\limits_{w} f(w)\\\\\n",
        "s.t. \\space\\space \n",
        "\\begin{matrix}\n",
        "g_i(w)\\le 0\\\\\n",
        "h_i(w)\\le 0\n",
        "\\end{matrix}\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "这种形式下，它的拉格朗日函数如下\n",
        "\n",
        "In this format, its Lagrange function is below\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(w,\\alpha,\\beta)=f(w)+\\sum_{i=1}^k \\alpha_i g_i(w)+\\sum_{i=1}^l \\beta_i h_i(w)\n",
        "$$\n",
        "\n",
        "此时，问题就变成了$\\mathop{max}\\limits_{\\alpha,\\beta:\\alpha\\ge0}\\mathcal{L}(w,\\alpha,\\beta)$​\n",
        "\n",
        "Now, the question becomes $\\mathop{max}\\limits_{\\alpha,\\beta:\\alpha\\ge0}\\mathcal{L}(w,\\alpha,\\beta)$\n",
        "\n",
        "同理，对于我们的SVM，也是相同的做法，我们先把原始优化变成primal dual的形式，也就是\n",
        "\n",
        "So, for SVM, we do the same thing. We need to make it prime dual first. It is like below.\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "min_{w,b}\\frac{1}{2}||w||^2\\\\\n",
        "s.t.  \\space 1-y_i(w^Tx_i+b)\\le 0, i=1,\\cdots, n\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "于是，它的拉格朗日函数变成了\n",
        "\n",
        "And its Lagrange function is below. \n",
        "\n",
        "$$\n",
        "\\mathcal{L}(w,b,\\alpha)=\\frac{1}{2}||w||^2 + \\sum_{i=1}^n \\alpha_i (1-y_i(w^Tx_i+b))\n",
        "$$\n",
        "\n",
        "此时，我们要对$w,b$​进行求导\n",
        "\n",
        "Then, we can get the partial derivative of $w,b$.\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "\\triangledown_w\\mathcal{L}(w,b,\\alpha)=w-\\sum_{i=1}^n \\alpha_i y_i x_i=0\\\\\n",
        "\\triangledown_b\\mathcal{L}(w,b,\\alpha)=-\\sum_{i=1}^n \\alpha_i y_i=0\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "于是，我们可以得到 $w=\\sum_{i=1}^n \\alpha_i y_i x_i$​ and $\\sum_{i=1}^n \\alpha_i y_i=0$​​. \n",
        "\n",
        "Now, we can get  $w=\\sum_{i=1}^n \\alpha_i y_i x_i$ and $\\sum_{i=1}^n \\alpha_i y_i=0$. \n",
        "\n",
        "然后我们再把$w$​的值给代回到拉格朗日函数中，可以得到\n",
        "\n",
        "And we put $w$ into the Lagrange function\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "\\mathcal{L}(w,b,\\alpha)=\\frac{1}{2}||\\sum_{i=1}^n \\alpha_i y_i x_i||^2+\\sum_{i=1}^n \\alpha_i(1-y_i((\\sum_{j=1}^n \\alpha_j y_j x_j)^Tx_i +b))\\\\\n",
        "=\\frac{1}{2}(\\sum_{i=1}^n \\alpha_i y_i x_i)^T (\\sum_{j=1}^n \\alpha_j y_j x_j)+\\sum_{i=1}^n \\alpha_i -\\sum_{i=1}^n \\alpha_i y_i(\\sum_{j=1}^n \\alpha_j y_j x_j)^Tx_i - \\sum_{i=1}^n \\alpha_i y_ib\\\\\n",
        "=\\frac{1}{2}\\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j x_i^T x_j+\\sum_{i=1}^n \\alpha_i -\\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j x_i^T x_j\\\\\n",
        "=\\sum_{i=1}^n \\alpha_i-\\frac{1}{2}\\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j x_i^T x_j\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "现在，我们函数中已经没有了$w,b$, 只剩下了$\\alpha$​. 所以，我们构造的对偶问题就是\n",
        "\n",
        "Now, in the function, we do not have $w,b$ and only $\\alpha$ left, so the dual problem we constructed is shown below\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "max_{\\alpha}= \\sum_{i=1}^n \\alpha_i-\\frac{1}{2}\\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j x_i^T x_j\\\\\n",
        "s.t. \\space \\space\n",
        "\\begin{matrix}\\alpha_i \\ge0\\\\\n",
        "\\sum_{i=1}^n \\alpha_i y_i=0\n",
        "\\end{matrix}\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "SVM的决策式子，也就是SVM的模型，就可以写成, 这里呢，每一个$x$，都会有一个对应的$\\alpha$​.\n",
        "\n",
        "SVM decision formula is shown below. We can know for every $x$, there is a corresponding $\\alpha$\n",
        "\n",
        "$$\n",
        "f(x)=w^Tx+b=\\sum_{i=1}^n \\alpha_i y_i x_i^T x +b\n",
        "$$\n",
        "\n",
        "这里，我们会有一个疑问，每次测试一个新样本，就需要跟每一个数据进行转置乘积并求和，哇，假设有一个亿的数据，那岂不是要有很大的空间来存储。看到这里，我们必须要清楚，我们的式子是有不等式约束的，也就是要满足KKT条件.\n",
        "\n",
        "Here, we may have a question. When we test a new test data, then we need to transpose every $x$​​ and get the cumulative sum. If we have a billion data, then we need a lot of space to store it. But, we need to know, our formula we got lastly satisfies the kKT conditions which is shown below\n",
        "\n",
        "$$\n",
        "\\begin{cases}\n",
        "\\alpha_i\\ge0 \\\\\n",
        "y_if(x_i)-1\\ge0\\\\\n",
        "\\alpha_i(y_if(x_i)-1)=0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "所以，我们是不用担心这么多数据的，根据KKT条件的第三行，可以得知非support vector的$\\alpha=0$. 只有support vector，$y_if(x_i)-1=0$​. 所以，这里就有一个SVM的重要性质，就是训练完成后，大部分的训练样本都不需要保留，最终模型只跟support vector相关。\n",
        "\n",
        "So, we do need to worry about it. According to the third line of KKT conditions, we can know that the $\\alpha$ value of non support vectors are all equal to 0. Therefore, there is a very important property of SVM that the final model only associates with support vectors and most of training data does not need to be stored.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkB8jGtZazTs"
      },
      "source": [
        "## SVM with slack variable\n",
        "\n",
        "我们的数据集中，是会有噪音的，造成很难线性完全可分。就像下面的这个例子，会相对来说比较难划分，所以我们可以用soft-margin.\n",
        "\n",
        "It is very normal that we have noise in the data. It will be very hard to be linearly separated. Let's look up the picture below, it will be hard to separate, so that we have to use soft-margin.\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/68700549/123352808-1c9a0580-d52e-11eb-8307-70dc6dc28303.png\" alt=\"WeChat Screenshot_20210624205225\" style=\"zoom:50%;\" />\n",
        "\n",
        "soft-margin的意思就是在满足maximum margin的同时，允许一些可出错的情况，但是要是这个可出错的情况尽可能少 。我们加一个slack variable，优化公式就变为\n",
        "\n",
        "soft-margin means when the line satisties the maximum margin, we allow the classifier to make some mistakes. But we try to avoid this mistakes as much as possible. We add a slack variable, so the optimize formula is:\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "\\mathop{min}\\limits_{w,b}\\frac{1}{2}||w||^2+C\\sum_{i=1}^m \\xi_i\\\\\n",
        "s.t. \\space \n",
        "\\begin{matrix}y_i(w^Tx_i+b)\\ge 1-\\xi_i\\\\\n",
        "\\xi_i\\ge0,i=1,2,...,m\n",
        "\\end{matrix}\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "加入 $\\xi$ 之后，我们就可以允错一些数据了，原来是要大于等于1，现在值变小了，大于等于$1-\\xi$就好了，所以会允错。每一个样本都会对应一个$\\xi$.因为我们还是要去求$w,b$,同时也是要优化$\\xi_i$的，所以要先转化成dual问题，第一步就是Lagrange 化\n",
        "\n",
        "After we added variable $\\xi$, it means we allow the classifier to make some mistakes. Originally, the formula only arrows >=1 and now, it becomes $1-\\xi_i$. So, the value becomes smaller. Every point will have a corresponding $\\xi_i$. We still need to get $w,b$ and at the same time, we need to optimize the $\\xi_i$ too. So, we can have it Lagrange format, that is\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(w,b,\\xi,\\alpha,\\mu)=\\frac{1}{2}||w||^2+C\\sum_{i=1}^m \\xi_i-\\sum_{i=1}^m \\alpha_i(y_i(w^Tx_i+b)+\\xi_i-1)-\\sum_{i=1}^m \\mu_i\\xi_i\n",
        "$$\n",
        "\n",
        "于是，我们需要对$w,b,\\xi$​进行求偏导得0，于是，我们就可以得到\n",
        "\n",
        "Now, we can get the partial derivative of $w,b,\\xi$​ and let them become 0\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "\\triangledown_w \\mathcal{L}(w,b,\\xi,\\alpha,\\mu)=w-\\sum_{i=1}^m \\alpha_i y_ix_i=0\\\\\n",
        "\\triangledown_b \\mathcal{L}(w,b,\\xi,\\alpha,\\mu)=\\sum_{i=1}^m \\alpha_iy_i=0\\\\\n",
        "\\triangledown_{\\xi_i} \\mathcal{L}(w,b,\\xi,\\alpha,\\mu)=C-\\alpha_i-\\mu_i=0\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "代入之后，可以得到\n",
        "\n",
        "we put what we got into the formula\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "\\mathcal{L}(w,b,\\xi,\\alpha,\\mu)=\\frac{1}{2}\\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i\\alpha_jy_iy_jx_i^T x_j-\\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i\\alpha_jy_iy_jx_i^T x_j+C\\sum_{i=1}^m \\xi_i -\\sum_{i=1}^m \\alpha_i \\xi_i-\\sum_{i=1}^m \\mu_i \\xi_i+\\sum_{i=1}^m \\alpha_i\\\\\n",
        "=\\sum_{i=1}^m \\alpha_i-\\frac{1}{2}\\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i\\alpha_jy_iy_jx_i^T x_j+\\sum_{i=1}^m \\xi_i (C-\\alpha_i-\\mu_i)\\\\\n",
        "=\\sum_{i=1}^m \\alpha_i-\\frac{1}{2}\\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i\\alpha_jy_iy_jx_i^T x_j\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "我们可以得到其对偶问题\n",
        "\n",
        "We can get the dual problem of SVM with slack variable\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "\\mathop{max}\\limits_{\\alpha} \\space \\sum_{i=1}^m a_i -\\frac{1}{2}\\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j x_i^Tx_j \\\\\n",
        "s.t. \\space\\space \n",
        "\\begin{matrix}\\sum_{i=1}^m a_i y_i=0 \\\\\n",
        "0\\le\\alpha_i\\le C,\\space\\space i=1,2,...,m\n",
        "\\end{matrix}\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "对于上面的公式，我们要知道$y_iy_j$,如果两个训练数据点属于同一类别会使值增加，否则减小。$x_i^Tx_j$是衡量两个数据点的相似性。再看$\\sum_{i=1}^m a_i y_i=0$. 不同数据点的$\\alpha$值会不一样，但是，对于不同类别(比如说，+1,-1,两种类别)，则权重一致，也就是属于+1样本的权重和，属于-1样本的权重和，这两个的绝对值会相等，因为要和为0.\n",
        "\n",
        "Here, we explain the above formula. $y_iy_j$ means if two training data point are the same class, it will increase the value, decrease otherwise. $x_i^Tx_j$ will measure the similarity of two data points. Let's see$\\sum_{i=1}^m a_i y_i=0$, different data points will have different $\\alpha_i$.\n",
        "\n",
        "对于soft-margin的KKT条件是\n",
        "The KKT conditions of soft-margin are\n",
        "\n",
        "$$\n",
        "\\begin{cases}\n",
        "\\alpha_i\\ge0,\\mu_i\\ge0 \\\\\n",
        "y_if(x_i)-1+\\xi_i\\ge0\\\\\n",
        "\\alpha_i(y_if(x_i)-1+\\xi_i)=0\\\\\n",
        "\\xi_i\\ge0,\\mu_i\\xi_i=0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "同样，soft-margin SVM也是只跟support vectors相关。加上slack variables之后，界限被放宽松了，support vector就不单单是在线上的，还包括一些允错范围内的数据点，也算是support vector.\n",
        "\n",
        "Same here, soft-margin SVM only associate with support vectors. After we added slack variable, it will relax the boundary. At this time, support vector is not only lying on teh line but also in the range of \"mistake zone\". \n",
        "\n",
        "这里我们要讨论一下这个variable C\n",
        "\n",
        "Here, we need to discuss the effect of variable C\n",
        "\n",
        "Effects of C: C值越大，margin越小.C 越高，容易过拟合。C 越小，容易欠拟合。\n",
        "\n",
        "Effects of C: larger C, smaller margin\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ImzO22NCBOt"
      },
      "source": [
        "## 带kernel的SVM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3n6qRiuCcVA"
      },
      "source": [
        "## SVM的smo解法\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWfp3ITRCgdj"
      },
      "source": [
        "## 使用SVM多类别分类"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6UyJh2PYjDL"
      },
      "source": [
        "# SVM人脸识别结合cross-validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnpVLVXxYrAG"
      },
      "source": [
        "# 模型评估方法和SVM做人脸识别"
      ]
    }
  ]
}