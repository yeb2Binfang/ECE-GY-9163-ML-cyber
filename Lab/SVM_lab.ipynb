{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVM_lab.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMyqj83aQhJr3ZAX3UkqpJK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeb2Binfang/ECE-GY-9163-ML-cyber/blob/main/Lab/SVM_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH1PlWb3E6Xa"
      },
      "source": [
        "# SVM review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "toc",
        "id": "9WHVp-_4EoUp"
      },
      "source": [
        ">[SVM review](#scrollTo=QH1PlWb3E6Xa)\n",
        "\n",
        ">[Basic knowledge of SVM](#scrollTo=sZiCfDOE3jsb)\n",
        "\n",
        ">>[The intuitive understanding of SVM](#scrollTo=kq9ky35Fa4jR)\n",
        "\n",
        ">>[The mathematic of SVM](#scrollTo=6hQUIt31e2eG)\n",
        "\n",
        ">>[SVM with slack variable](#scrollTo=VkB8jGtZazTs)\n",
        "\n",
        ">>[SVM with kernel](#scrollTo=4ImzO22NCBOt)\n",
        "\n",
        ">>[SVM的smo解法](#scrollTo=y3n6qRiuCcVA)\n",
        "\n",
        ">>[使用SVM多类别分类](#scrollTo=GWfp3ITRCgdj)\n",
        "\n",
        ">[SVM人脸识别结合cross-validation](#scrollTo=m6UyJh2PYjDL)\n",
        "\n",
        ">[模型评估方法和SVM做人脸识别](#scrollTo=BnpVLVXxYrAG)\n",
        "\n",
        ">[The interview questions of SVM](#scrollTo=vuxb08Nocv7J)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZiCfDOE3jsb"
      },
      "source": [
        "# Basic knowledge of SVM\n",
        "\n",
        "在ML cyber这门课上，教授给我们review了SVM。我们需要了解SVM是怎么work的，也需要知道其数学过程。在我们了解其过程之后，我们会做一个小lab来加深对其的理解。\n",
        "\n",
        "In Machine Learning Cyber class, we reviewed SVM. We need to know how does SVM work and understand its mathematical process also. After review them, we will do a small project to have a deep understanding \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq9ky35Fa4jR"
      },
      "source": [
        "## The intuitive understanding of SVM\n",
        "\n",
        "Now, we have two classes that can be linearly separated. Below figure shows the case we are assuming.\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/68700549/134811043-62c61f8e-38fd-492e-bff7-07e7d2a2c77b.png\" alt=\"WeChat Screenshot_20210926095925\" style=\"zoom:10%;\" />\n",
        "\n",
        "现在呢，我们可以看到有两个类，可以被线性分开，图中我们也可以看到有两条线都可以将其分开，但是哪一条更好呢？\n",
        "\n",
        "Now, we can that there are two lines can linearly separated the classess. But which one is better? \n",
        "\n",
        "\n",
        "![WeChat Screenshot_20210926100557](https://user-images.githubusercontent.com/68700549/134811213-072c5509-c815-489c-a52d-7efe36e72470.png)\n",
        "\n",
        "于是就有了SVM，我们希望能找到一条线，就是找到两个类的边界，然后我们找到的线可以使得到这两条线的距离相等。就如SVM的图所示.\n",
        "\n",
        "SVM, support vector machine, will help find a line that can linearly separated them well. The SVM will find the boundary of both classes and the line will be posited between them. We hope the distance between both classes' boundary points can be equal. Look at the graph above.\n",
        "\n",
        "假设能找到线，我们就希望能够最大化这个距离，让两个classes尽可能地分开。如果有新数据进来，那么我们找到的线也是能够尽可能地正确分类新数据。\n",
        "\n",
        "We try to maximize the distance of the margin to separate two classes as much as possible. If new data comes in, the line has much higher probability to classifier it well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hQUIt31e2eG"
      },
      "source": [
        "## The mathematic of SVM\n",
        "\n",
        "现在我们来算一下穿过support vector (两个classes的边界点)的两条线之间的距离是多少，从图中，我们可以看到有两条线，一个是$wx+b=1$, 另一条是$wx+b=-1$. 这里先解释下为什么是1和-1，即使是其他的数，我们也是可以变换到1和-1来的，就是做乘除法而已。\n",
        "\n",
        "Now, we are calculating the distance between two lines that pass throught the support vecotrs (show in the graph). We can see there are two dashed line, one is $wx+b=1$, another one is $wx+b=-1$. The reason why it is 1 and -1. It actually does not matter. We can transform it to 1 and -1 even though they are not. Just do some multiplication and division. Using 1 and -1 is much easier. \n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/68700549/134814170-94381088-5f22-4ba3-910b-92a523d453d8.png\" alt=\"WeChat Screenshot_20210926112656\" style=\"zoom:50%;\" />\n",
        "\n",
        "这两条线的距离其实就是$d_1+d_2$​​​. 我们需要知道点到线的距离的计算公式，如下\n",
        "\n",
        "The distance between these two lines is $d_1+d_2$. The way to calculate the distance from a point to a line is shown below.​\n",
        "$$\n",
        "r = \\frac{|wx+b|}{||w||_2}\n",
        "$$\n",
        "\n",
        "现在，我们要计算$d_1+d_2$​, 也就是要看support vector，那么也就是\n",
        "\n",
        "Now, we need to calculate the $d_1+d_2$. So, we need to look at the support vectors. Then we get the distance.​\n",
        "\n",
        "$$\n",
        "\\gamma = d_1+d_2=\\frac{|wx_1+b|+|wx_2+b|}{||w||_2}=\\frac{2}{||w||_2}\n",
        "$$\n",
        "\n",
        "然后，现在我们想要最大化这个距离.于是我们就要做一个约束优化. 上面的$\\frac{2}{||w||_2}$可以写成下面的形式. $y_i(w^Tx_i+b)\\ge 1$, 也就是分类正确的意思\n",
        "\n",
        "We want to maximize the distance. We can do a constraint optimization. $y_i(w^Tx_i+b)\\ge 1$ means classify points correctly.\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "min_{w,b}\\frac{1}{2}||w||^2\\\\\n",
        "s.t.  \\space y_i(w^Tx_i+b)\\ge 1, i=1,\\cdots, n\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "遇到约束优化，我们就可以进行拉格朗日化，但前提要变成对偶条件。也就是primal dual，对于原优化问题\n",
        "\n",
        "when we have constraint optimization, we can think about Lagrange multipliers. We should make the optimization become primal dual problem. We should make things like below\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "\\mathop{min}\\limits_{w} f(w)\\\\\n",
        "s.t. \\space\\space \n",
        "\\begin{matrix}\n",
        "g_i(w)\\le 0\\\\\n",
        "h_i(w)\\le 0\n",
        "\\end{matrix}\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "这种形式下，它的拉格朗日函数如下\n",
        "\n",
        "In this format, its Lagrange function is below\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(w,\\alpha,\\beta)=f(w)+\\sum_{i=1}^k \\alpha_i g_i(w)+\\sum_{i=1}^l \\beta_i h_i(w)\n",
        "$$\n",
        "\n",
        "此时，问题就变成了$\\mathop{max}\\limits_{\\alpha,\\beta:\\alpha\\ge0}\\mathcal{L}(w,\\alpha,\\beta)$​\n",
        "\n",
        "Now, the question becomes $\\mathop{max}\\limits_{\\alpha,\\beta:\\alpha\\ge0}\\mathcal{L}(w,\\alpha,\\beta)$\n",
        "\n",
        "同理，对于我们的SVM，也是相同的做法，我们先把原始优化变成primal dual的形式，也就是\n",
        "\n",
        "So, for SVM, we do the same thing. We need to make it prime dual first. It is like below.\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "min_{w,b}\\frac{1}{2}||w||^2\\\\\n",
        "s.t.  \\space 1-y_i(w^Tx_i+b)\\le 0, i=1,\\cdots, n\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "于是，它的拉格朗日函数变成了\n",
        "\n",
        "And its Lagrange function is below. \n",
        "\n",
        "$$\n",
        "\\mathcal{L}(w,b,\\alpha)=\\frac{1}{2}||w||^2 + \\sum_{i=1}^n \\alpha_i (1-y_i(w^Tx_i+b))\n",
        "$$\n",
        "\n",
        "此时，我们要对$w,b$​进行求导\n",
        "\n",
        "Then, we can get the partial derivative of $w,b$.\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "\\triangledown_w\\mathcal{L}(w,b,\\alpha)=w-\\sum_{i=1}^n \\alpha_i y_i x_i=0\\\\\n",
        "\\triangledown_b\\mathcal{L}(w,b,\\alpha)=-\\sum_{i=1}^n \\alpha_i y_i=0\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "于是，我们可以得到 $w=\\sum_{i=1}^n \\alpha_i y_i x_i$​ and $\\sum_{i=1}^n \\alpha_i y_i=0$​​. \n",
        "\n",
        "Now, we can get  $w=\\sum_{i=1}^n \\alpha_i y_i x_i$ and $\\sum_{i=1}^n \\alpha_i y_i=0$. \n",
        "\n",
        "然后我们再把$w$​的值给代回到拉格朗日函数中，可以得到\n",
        "\n",
        "And we put $w$ into the Lagrange function\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "\\mathcal{L}(w,b,\\alpha)=\\frac{1}{2}||\\sum_{i=1}^n \\alpha_i y_i x_i||^2+\\sum_{i=1}^n \\alpha_i(1-y_i((\\sum_{j=1}^n \\alpha_j y_j x_j)^Tx_i +b))\\\\\n",
        "=\\frac{1}{2}(\\sum_{i=1}^n \\alpha_i y_i x_i)^T (\\sum_{j=1}^n \\alpha_j y_j x_j)+\\sum_{i=1}^n \\alpha_i -\\sum_{i=1}^n \\alpha_i y_i(\\sum_{j=1}^n \\alpha_j y_j x_j)^Tx_i - \\sum_{i=1}^n \\alpha_i y_ib\\\\\n",
        "=\\frac{1}{2}\\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j x_i^T x_j+\\sum_{i=1}^n \\alpha_i -\\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j x_i^T x_j\\\\\n",
        "=\\sum_{i=1}^n \\alpha_i-\\frac{1}{2}\\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j x_i^T x_j\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "现在，我们函数中已经没有了$w,b$, 只剩下了$\\alpha$​. 所以，我们构造的对偶问题就是\n",
        "\n",
        "Now, in the function, we do not have $w,b$ and only $\\alpha$ left, so the dual problem we constructed is shown below\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "max_{\\alpha}= \\sum_{i=1}^n \\alpha_i-\\frac{1}{2}\\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j x_i^T x_j\\\\\n",
        "s.t. \\space \\space\n",
        "\\begin{matrix}\\alpha_i \\ge0\\\\\n",
        "\\sum_{i=1}^n \\alpha_i y_i=0\n",
        "\\end{matrix}\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "SVM的决策式子，也就是SVM的模型，就可以写成, 这里呢，每一个$x$，都会有一个对应的$\\alpha$​.\n",
        "\n",
        "SVM decision formula is shown below. We can know for every $x$, there is a corresponding $\\alpha$\n",
        "\n",
        "$$\n",
        "f(x)=w^Tx+b=\\sum_{i=1}^n \\alpha_i y_i x_i^T x +b\n",
        "$$\n",
        "\n",
        "这里，我们会有一个疑问，每次测试一个新样本，就需要跟每一个数据进行转置乘积并求和，哇，假设有一个亿的数据，那岂不是要有很大的空间来存储。看到这里，我们必须要清楚，我们的式子是有不等式约束的，也就是要满足KKT条件.\n",
        "\n",
        "Here, we may have a question. When we test a new test data, then we need to transpose every $x$​​ and get the cumulative sum. If we have a billion data, then we need a lot of space to store it. But, we need to know, our formula we got lastly satisfies the kKT conditions which is shown below\n",
        "\n",
        "$$\n",
        "\\begin{cases}\n",
        "\\alpha_i\\ge0 \\\\\n",
        "y_if(x_i)-1\\ge0\\\\\n",
        "\\alpha_i(y_if(x_i)-1)=0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "所以，我们是不用担心这么多数据的，根据KKT条件的第三行，可以得知非support vector的$\\alpha=0$. 只有support vector，$y_if(x_i)-1=0$​. 所以，这里就有一个SVM的重要性质，就是训练完成后，大部分的训练样本都不需要保留，最终模型只跟support vector相关。\n",
        "\n",
        "So, we do need to worry about it. According to the third line of KKT conditions, we can know that the $\\alpha$ value of non support vectors are all equal to 0. Therefore, there is a very important property of SVM that the final model only associates with support vectors and most of training data does not need to be stored.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkB8jGtZazTs"
      },
      "source": [
        "## SVM with slack variable\n",
        "\n",
        "我们的数据集中，是会有噪音的，造成很难线性完全可分。就像下面的这个例子，会相对来说比较难划分，所以我们可以用soft-margin.\n",
        "\n",
        "It is very normal that we have noise in the data. It will be very hard to be linearly separated. Let's look up the picture below, it will be hard to separate, so that we have to use soft-margin.\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/68700549/123352808-1c9a0580-d52e-11eb-8307-70dc6dc28303.png\" alt=\"WeChat Screenshot_20210624205225\" style=\"zoom:50%;\" />\n",
        "\n",
        "soft-margin的意思就是在满足maximum margin的同时，允许一些可出错的情况，但是要是这个可出错的情况尽可能少 。我们加一个slack variable，优化公式就变为\n",
        "\n",
        "soft-margin means when the line satisties the maximum margin, we allow the classifier to make some mistakes. But we try to avoid this mistakes as much as possible. We add a slack variable, so the optimize formula is:\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "\\mathop{min}\\limits_{w,b}\\frac{1}{2}||w||^2+C\\sum_{i=1}^m \\xi_i\\\\\n",
        "s.t. \\space \n",
        "\\begin{matrix}y_i(w^Tx_i+b)\\ge 1-\\xi_i\\\\\n",
        "\\xi_i\\ge0,i=1,2,...,m\n",
        "\\end{matrix}\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "加入 $\\xi$ 之后，我们就可以允错一些数据了，原来是要大于等于1，现在值变小了，大于等于$1-\\xi$就好了，所以会允错。每一个样本都会对应一个$\\xi$.因为我们还是要去求$w,b$,同时也是要优化$\\xi_i$的，所以要先转化成dual问题，第一步就是Lagrange 化\n",
        "\n",
        "After we added variable $\\xi$, it means we allow the classifier to make some mistakes. Originally, the formula only arrows >=1 and now, it becomes $1-\\xi_i$. So, the value becomes smaller. Every point will have a corresponding $\\xi_i$. We still need to get $w,b$ and at the same time, we need to optimize the $\\xi_i$ too. So, we can have it Lagrange format, that is\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(w,b,\\xi,\\alpha,\\mu)=\\frac{1}{2}||w||^2+C\\sum_{i=1}^m \\xi_i-\\sum_{i=1}^m \\alpha_i(y_i(w^Tx_i+b)+\\xi_i-1)-\\sum_{i=1}^m \\mu_i\\xi_i\n",
        "$$\n",
        "\n",
        "于是，我们需要对$w,b,\\xi$​进行求偏导得0，于是，我们就可以得到\n",
        "\n",
        "Now, we can get the partial derivative of $w,b,\\xi$​ and let them become 0\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "\\triangledown_w \\mathcal{L}(w,b,\\xi,\\alpha,\\mu)=w-\\sum_{i=1}^m \\alpha_i y_ix_i=0\\\\\n",
        "\\triangledown_b \\mathcal{L}(w,b,\\xi,\\alpha,\\mu)=\\sum_{i=1}^m \\alpha_iy_i=0\\\\\n",
        "\\triangledown_{\\xi_i} \\mathcal{L}(w,b,\\xi,\\alpha,\\mu)=C-\\alpha_i-\\mu_i=0\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "代入之后，可以得到\n",
        "\n",
        "we put what we got into the formula\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "\\mathcal{L}(w,b,\\xi,\\alpha,\\mu)=\\frac{1}{2}\\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i\\alpha_jy_iy_jx_i^T x_j-\\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i\\alpha_jy_iy_jx_i^T x_j+C\\sum_{i=1}^m \\xi_i -\\sum_{i=1}^m \\alpha_i \\xi_i-\\sum_{i=1}^m \\mu_i \\xi_i+\\sum_{i=1}^m \\alpha_i\\\\\n",
        "=\\sum_{i=1}^m \\alpha_i-\\frac{1}{2}\\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i\\alpha_jy_iy_jx_i^T x_j+\\sum_{i=1}^m \\xi_i (C-\\alpha_i-\\mu_i)\\\\\n",
        "=\\sum_{i=1}^m \\alpha_i-\\frac{1}{2}\\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i\\alpha_jy_iy_jx_i^T x_j\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "我们可以得到其对偶问题\n",
        "\n",
        "We can get the dual problem of SVM with slack variable\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "\\mathop{max}\\limits_{\\alpha} \\space \\sum_{i=1}^m a_i -\\frac{1}{2}\\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j x_i^Tx_j \\\\\n",
        "s.t. \\space\\space \n",
        "\\begin{matrix}\\sum_{i=1}^m a_i y_i=0 \\\\\n",
        "0\\le\\alpha_i\\le C,\\space\\space i=1,2,...,m\n",
        "\\end{matrix}\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "对于上面的公式，我们要知道$y_iy_j$,如果两个训练数据点属于同一类别会使值增加，否则减小。$x_i^Tx_j$是衡量两个数据点的相似性。再看$\\sum_{i=1}^m a_i y_i=0$. 不同数据点的$\\alpha$值会不一样，但是，对于不同类别(比如说，+1,-1,两种类别)，则权重一致，也就是属于+1样本的权重和，属于-1样本的权重和，这两个的绝对值会相等，因为要和为0.\n",
        "\n",
        "Here, we explain the above formula. $y_iy_j$ means if two training data point are the same class, it will increase the value, decrease otherwise. $x_i^Tx_j$ will measure the similarity of two data points. Let's see$\\sum_{i=1}^m a_i y_i=0$, different data points will have different $\\alpha_i$.\n",
        "\n",
        "对于soft-margin的KKT条件是\n",
        "The KKT conditions of soft-margin are\n",
        "\n",
        "$$\n",
        "\\begin{cases}\n",
        "\\alpha_i\\ge0,\\mu_i\\ge0 \\\\\n",
        "y_if(x_i)-1+\\xi_i\\ge0\\\\\n",
        "\\alpha_i(y_if(x_i)-1+\\xi_i)=0\\\\\n",
        "\\xi_i\\ge0,\\mu_i\\xi_i=0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "同样，soft-margin SVM也是只跟support vectors相关。加上slack variables之后，界限被放宽松了，support vector就不单单是在线上的，还包括一些允错范围内的数据点，也算是support vector.\n",
        "\n",
        "Same here, soft-margin SVM only associate with support vectors. After we added slack variable, it will relax the boundary. At this time, support vector is not only lying on teh line but also in the range of \"mistake zone\". \n",
        "\n",
        "这里我们要讨论一下这个variable C\n",
        "\n",
        "Here, we need to discuss the effect of variable C\n",
        "\n",
        "Effects of C: C值越大，margin越小.C 越高，容易过拟合。C 越小，容易欠拟合。\n",
        "\n",
        "Effects of C: larger C, smaller margin, easy to overfit. smaller C larger margin, easy to underfit.\n",
        "\n",
        "我们来看下下面这张图，可以看到C值越大，margin越小，C值越小，margin越大\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/68700549/123664644-83a50c00-d805-11eb-9a1a-e7cd4805bd62.png\" alt=\"WeChat Screenshot_20210628113916\" style=\"zoom:50%;\" />\n",
        "\n",
        "参数C越大，支持向量的数量越少.当 C 逐渐增大的时候，判定边界也越来越复杂，过拟合的风险越来越大，同时，我们也发现支持向量（白色边框的点）的数量越来越少。这是因为当 C 增大时，对于误差的惩罚增大，判定边界趋向于将每一个点都正确地分类，导致支持向量机的margin越来越窄，从而使得能成为支持向量的点的数量越来越少。\n",
        "\n",
        "The larger C, the less support vectors. When we increase C value, the decision boundary becomes more complexity,which will result in overfitting. Meanwhile, we notice that the number of support vectors decrease because larger C will increase the penality of error. The decision boundary tries to classify every point correctly. The smaller margin, the less support vectors.\n",
        "\n",
        "我们看下面这张图，C越大，它的决策边界就越复杂，容易造成overfitting\n",
        "\n",
        "we can look at the below picture, larger C has a more complex decision boundary \n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/68700549/124841610-a51b9b80-df5b-11eb-82d3-e10b747eebb5.jpg\" alt=\"v2-285f04e1e50ed9a840e4e7a0f19032c5_r\" style=\"zoom:67%;\" />\n",
        "\n",
        "对参数C的总结\n",
        "* smaller C\n",
        "\n",
        "    *  will have large margin\n",
        "    *  allow more violations of margin\n",
        "    *  more support vectors\n",
        "    *  reduce variance\n",
        "\n",
        "\n",
        "* larger C\n",
        "    *  small margin\n",
        "    *  reduce violations and fewer support vectors\n",
        "    *  highly fit to the data. Low bias, and higher variance\n",
        "    *  more chance to overfit\n",
        "\n",
        "\n",
        "接下来讲解一下Hinge loss，SVM里面用的就是Hinge loss。先来看看我们通常用的loss有哪些\n",
        "\n",
        "Later, we will explain the hinge loss which is used in SVM. Let's take a look the loss functions we often use.\n",
        "\n",
        "常用loss function\n",
        "*  Zero-one loss\n",
        "*  Hinge loss: $l_{hinge}(z)=max(0,1-z)$\n",
        "*  Exponential loss: $l_{exp}(z)=exp(-z)$\n",
        "*  Logistic loss: $l_{log}(z)=log(1+exp(-z))$\n",
        "\n",
        "然后我们再看看下每个loss的图像是怎么样的\n",
        "\n",
        "And we need to know the graph of each loss functions above\n",
        "\n",
        "<img width=\"792\" alt=\"lossfunction\" src=\"https://user-images.githubusercontent.com/68700549/123355139-0b9fc300-d533-11eb-8d59-56fbdc332985.png\" style=\"zoom:50%;\" >\n",
        "\n",
        "我们这里从Hinge loss的角度来看$\\xi$, 也就是我们允错，error，我们的$\\xi_i\\ge 0$.\n",
        "\n",
        "From Hinge loss's view, we may have a good understanding of $\\xi$. We know that $\\xi_i \\ge 0$\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "y_i(w^Tx_i+b)\\ge1-\\xi_i\\\\\n",
        "\\downarrow\\\\\n",
        "\\xi_i\\ge 1-y_i(w^Tx_i+b)\\\\\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "所以，我们可以总结出\n",
        "\n",
        "So, we can conclude ethat \n",
        "\n",
        "$$\n",
        "\\xi_i=max(0,1-y_i(w^Tx_i+b))\n",
        "$$\n",
        "\n",
        "但是SVM这里为什么要用hinge loss呢？\n",
        "\n",
        "But why SVM prefers hinge loss?\n",
        "\n",
        "使用hinge loss后，仍旧保持了稀疏性。我们先看logistic loss，它在后面是不断趋近于0的，但是不等于0，即使SVM分类正确，仍旧有loss，因为不等于0. SVM在1之后，就全部等于0了，也就是分类正确之后，就把那个loss变为0了，这也会有个好处，就是对outlier不敏感，也就是说，我们看logistic，都是在分类正确的前提下，假设分类得好，loss就小，分类得相对差点，loss就大。而Hinge的话就一视同仁，也是在分类正确的前提下，不管你有多好，loss都是0，这也使得SVM的解具有稀疏性. 而且，在假设分类不正确的情况下，因为我们最后是要求导的，从而update这个$w,b$的，我们看logistic，如果分类不正确，越不正确，loss就越大，在gradient的过程中，稍微调一点，变化就很大。而hinge依旧是一视同仁，loss，分得越差，loss越大，但是求导后都是-1.\n",
        "\n",
        "After the SVM used hinge loss, it still can keep the sparsity. Let's take a look at the logistic loss, it will close to 0 but not equal to 0, which means that even if we classify the points correctly, there is still having loss.  But if we use hinge loss, we can see the graph that the loss is equal to 0 after x=1. It means if we classify points correctly, there is no loss any more. For logistic, it will have a small loss if the model classify it well and a large loss otherwise. But Hinge loss is different because it will view all misclassification in a same way. Whatever how well the model classifies, the loss is 0 and because of it, the model has the sparsity. If the model misclassify the points, for logistic, the partial derivative will be very large if the model classifies very bad, for hinge loss, the partial derivative will be same, all of them will be -1. If it is very large, when we update the variable $w,b$, it will update too much which may miss the optimizaiton point.\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/68700549/123664015-f6fa4e00-d804-11eb-91d3-fa7cab702f19.png\" alt=\"WeChat Screenshot_20210628113531\" style=\"zoom:50%;\" />\n",
        "\n",
        "Hinge loss的优点\n",
        "\n",
        "* Convex function，容易优化\n",
        "* 在自变量小于0的部分，梯度较小(-1),对错误分类的penalty较小\n",
        "* 在自变量大于0的部分，值为0.只要对某个数据分类是正确的，即使分类正确的可能性足够高，也不用针对这个数据进一步优化了\n",
        "* 在自变量为0处不可求导，需要分段进行求导\n",
        "* 求解最优化时，只有support vectors是确定分界线，而且support vector的个数是远远小于训练数据的个数的\n",
        "\n",
        "The advantages of hinge loss\n",
        "\n",
        "* it is a convex function, easy to optimize\n",
        "* for misclassification part, the partial derivaive is -1, it will not penalize a lot\n",
        "* It will not have a further optimizaiton if it classifies correctly because the partial derivative is 0.\n",
        "* The partial derivative at point 0 has to separate\n",
        "* When we optimization the model, only support vectors can decide the decision boundary and the number of support vectors is far less than the number of training data.\n",
        "\n",
        "Logistic loss的优势主要在于其输出具有自然的概率意义，就是在给出预测标记的用时也给出了概率，能直接用于multi-classification。但是，不具有稀疏性，且依赖于更多的训练样本，预测开销大\n",
        "\n",
        "The main advantage of hinge loss is that it can give us the classification probability and it can be applied to multi-classification firectly. However, it does not have the sparsity property and it needs to depend on training data. It needs a lot of memory too."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ImzO22NCBOt"
      },
      "source": [
        "## SVM with kernel\n",
        "\n",
        "我们现在的数据集中也许不存在一个能够正确划分样本的超平面，就比如说下图，不能够线性划分，需要投射到高维去划分数据。投射之后，样本就可能在这个特征空间内线性可分。如果原始空间是有限维，就是feature数有限，那么是一定存在一个高维特征空间使得样本线性可分。\n",
        "\n",
        "In reality, the data probably cannot be linearly separated. So we need to project the data to a high dimension. After the projection, we can find a hyperplane to linear separate the data. There is a high dimension that can separate the data.\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/68700549/123436366-8e109d00-d59c-11eb-8b27-868423c86372.png\" alt=\"WeChat Screenshot_20210625100300\" style=\"zoom:50%;\" />\n",
        "\n",
        "但是投射到高维也是存在一定的缺点，会增大计算量，本来是二维的，投射到了三维甚至更高维。计算量增大。我们也知道，维度越高，需要训练的数据也要越多。\n",
        "\n",
        "It will increase computation if we project data to a high dimension. Originally, it is two dimension, now, it is in three dimension or even higher. We know that high dimensional data requires more training data.\n",
        "\n",
        "先来看看数学模型。现在我们假设把数据$x$映射到高维中，也就是$\\phi(x)$，我们的$x$在高维空间的投射.于是，我们模型就可以表示为\n",
        "\n",
        "Let's take a look at the mathematic model. If we project the data $x$ to a high dimension, i.e. it is $\\phi(x)$. The model can be represented as \n",
        "\n",
        "$$\n",
        "f(x)=w^T\\phi(x)+b\n",
        "$$\n",
        "\n",
        "还是跟上面一样，我们要最大化margin，就是求\n",
        "\n",
        "It is the same as the above, we need to maximize the margin\n",
        "\n",
        "$$\n",
        "\\mathop{min}\\limits_{w,b}\\frac{1}{2}||w||^2\\\\\n",
        "\\text{s.t. }y_i(w^T\\phi(x_i)+b)\\ge1,i=1,2,...,m\n",
        "$$\n",
        "\n",
        "然后，我们就可以得到dual 问题\n",
        "\n",
        "Then we can get the dual problem\n",
        "\n",
        "$$\n",
        "\\mathop{max}\\limits_{\\alpha}\\sum_{i=1}^m \\alpha_i - \\frac{1}{2}\\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i \\alpha_j y_iy_j\\phi(x_i)^T\\phi(x_j)\\\\\n",
        "\\text{s.t. }\\sum_{i=1}^m\\alpha_iy_i=0,\\\\\n",
        "\\alpha_i\\ge0,i=1,2,...,m\n",
        "$$\n",
        "\n",
        "但是呢，现在会有一个问题，就是投射到高维空间后可能是真的很高维，这就导致$\\phi(x_i)^T\\phi(x_j)$,计算非常困难，而且，找$\\phi$ 也不好找。于是呢，我们就设想有这么一个函数，也就是核函数,下面这个公式的意思就是使用一个核函数，使得$x_i,x_j$在高维空间的内积，也就是$<\\phi(x_i),\\phi(x_j)>$要等于通过核函数对$x_i,x_j$的计算结果。这个理解非常重要，满足这个条件，就被成为kernel trick\n",
        "\n",
        "However, if we project the data to a very high dimension, it will result in $\\phi(x_i)^T\\phi(x_j)$ computational expensive. Covarience matrix is always computational expensive. Meanwhile, it is hard to find $\\phi$. So, we may turn to a new function which is kernel function. It means that the inner product of $<\\phi(x_i),\\phi(x_j)>$ is equal to the kernel function with $x_i,x_j$. We need to understand it. When satisfy this condition, it may become a kernel function. The kernel trick is shown below\n",
        "\n",
        "$$\n",
        "k(x_i,x_j)=<\\phi(x_i),\\phi(x_j)>=\\phi(x_i)^T\\phi(x_j)\n",
        "$$\n",
        "\n",
        "假设我们有了核函数，我们的dual问题就可以重写为\n",
        "\n",
        "If we have kernel function now, then we dual format is shown below\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "\\mathop{max}\\limits_{\\alpha}\\sum_{i=1}^m \\alpha_i - \\frac{1}{2}\\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i \\alpha_j y_iy_jk(x_i,x_j)\\\\\n",
        "\\text{s.t. }\\sum_{i=1}^m\\alpha_iy_i=0,\\\\\n",
        "\\alpha_i\\ge0,i=1,2,...,m\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "那么，什么样的函数能成为核函数呢？条件是什么？\n",
        "\n",
        "What type of function can be a kernel function? What are the conditions?\n",
        "\n",
        "在与核函数$k(·,·)$做出运算后的矩阵,$G_{ij}=k(x_i,x_j)$，必须是对称矩阵，而且一定是半正定矩阵(positive semidefinite matrix)。半正定矩阵定义是，对于任意不为0的实列向量$x$,都有$x^TGx\\ge0$. 满足这两个条件的就可当做核函数使用。\n",
        "\n",
        "Two conditions. After we calculate with the kernel, the matrix must be a symmetric matrix and semi-positive definte (The leading principal minor are greater than or equal to 0). \n",
        "\n",
        "我们希望我们的数据能在高维空间中线性可分，所以kernel function的选择是至关重要的，这也是影响SVM性能的最大变数。\n",
        "\n",
        "We hope the data projected in high dimension can be linearly separated so that the selection of kernel function is quite important. The SVM performance will be heavily depend on it.\n",
        "\n",
        "核函数的三个重要性质：\n",
        "\n",
        "* 若$k_1,k_2$都是kernel function，则对于任意正数$\\gamma_1,\\gamma_2$,其线性组合,$\\gamma_1 k_1 + \\gamma_2 k_2$也是核函数\n",
        "* 若$k_1,k_2$都是kernel function，则函数的直积，$k_1 \\otimes k_2 (x,z)=k_1(x,z)k_2(x,z)$ 也是核函数\n",
        "* 若$k_1$是kernel function，则对于任意函数$g(x)$, $k(x,z)=g(x)k_1(x,z)g(z)$ 也是核函数\n",
        "\n",
        "Three properties of kernel function\n",
        "* if $k_1,k_2$ are kernel functions, then the linear combination of them are also kernel funciton, for all $\\gamma_1,\\gamma_2$, $\\gamma_1 k_1 + \\gamma_2 k_2$ is kernel function also\n",
        "\n",
        "* if $k_1,k_2$ are kernel functions, then the direct product of them is also kernel function. $k_1 \\otimes k_2 (x,z)=k_1(x,z)k_2(x,z)$\n",
        "* if $k_1$ is a kernel function, the for any function $g(x)$, $k(x,z)=g(x)k_1(x,z)g(z)$ is also kernel function\n",
        "\n",
        "这里列出常用的几个核函数\n",
        "\n",
        "The table shows the kernel function we use often. The RBF is very very important.\n",
        "\n",
        "![WeChat Screenshot_20210927214240](https://user-images.githubusercontent.com/68700549/135008486-21a2baaa-9f3f-4cf8-9ed5-4e39a51dff08.png)\n",
        "\n",
        "如果使用Sigmoid kernel，那么SVM就相当于没有hidden layer的神经网络。\n",
        "\n",
        "Let's talk about Gaussian kernel (RBF, radical basis function kernel)\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/68700549/123446786-36c3fa00-d5a7-11eb-92a3-0e3de0747de6.png\" alt=\"WeChat Screenshot_20210625111922\" style=\"zoom: 67%;\" />\n",
        "\n",
        "对Gaussian Kernel,我们在使用的时候，要讨论一下$\\gamma$. Gaussian kernel: $k(x_i,x_j)=exp(-\\frac{||x_i-x_j||^2}{2\\sigma^2})=exp(-\\frac{||x_i-x_j||^2}{\\gamma}),\\gamma=\\frac{1}{2\\sigma^2}$ . 我们看到当$\\gamma$很大的时候，很容易造成overfitting\n",
        "\n",
        "When we use gaussian kernel, we need to undersatnd the effect of $\\gamma$. Larger $\\gamma$ will result in overfitting.\n",
        "\n",
        "Effects of $\\gamma$:\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/68700549/123666132-f06cd600-d806-11eb-9a03-6d24a1a1ea90.png\" alt=\"WeChat Screenshot_20210628114938\" style=\"zoom:50%;\" />\n",
        "\n",
        "RBF 核函数的参数$\\gamma$定义了单个样本的影响波及范围，gamma 比较小的话，其影响较小；gamma 比较大的话，影响范围较大。gamma 越大，支持向量越少，gamma 越小，支持向量越多。支持向量的个数影响训练和预测的速度。gamma的物理意义，大家提到很多的RBF的幅宽，它会影响每个支持向量对应的高斯的作用范围，从而影响泛化性能。我的理解：如果gamma设的太大，$\\sigma$会很小，很小的高斯分布长得又高又瘦， 会造成只会作用于支持向量样本附近，对于未知样本分类效果很差，存在训练准确率可以很高，(如果让无穷小，则理论上，高斯核的SVM可以拟合任何非线性数据，但容易过拟合)；而如果设的过小，则会造成平滑效应太大，无法在训练集上得到特别高的准确率，也会影响测试集的准确率。\n",
        "\n",
        "在使用Gaussian kernel之前，要将数据standardization\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/68700549/124841329-f7a88800-df5a-11eb-826b-c9f089e47685.jpg\" alt=\"v2-b65393d97412927f7430ea474b19c853_r\" style=\"zoom:67%;\" />\n",
        "\n",
        "对$\\gamma$ 参数的总结\n",
        "* When we increase $\\gamma$\n",
        "    * classifier fits training data better\n",
        "    * classification region more complex\n",
        "    * lower bias error\n",
        "    * higher variance error\n",
        "    * reduce the number of support vectors\n",
        "\n",
        "\n",
        "Now, we will take a look the heatmap to analyze the effect of parameter C and $\\gamma$\n",
        "\n",
        "![WeChat Screenshot_20210927225106](https://user-images.githubusercontent.com/68700549/135014686-eefa545c-24b1-4330-ac89-69352c0ab086.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3n6qRiuCcVA"
      },
      "source": [
        "## SVM的smo解法\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWfp3ITRCgdj"
      },
      "source": [
        "## 使用SVM多类别分类"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6UyJh2PYjDL"
      },
      "source": [
        "# SVM人脸识别结合cross-validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnpVLVXxYrAG"
      },
      "source": [
        "# 模型评估方法和SVM做人脸识别"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuxb08Nocv7J"
      },
      "source": [
        "# The interview questions of SVM"
      ]
    }
  ]
}