{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVM_lab.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPTK9MWLpgNNxgU00Pbs0BL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeb2Binfang/ECE-GY-9163-ML-cyber/blob/main/Lab/SVM_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZiCfDOE3jsb"
      },
      "source": [
        "# Basic knowledge of SVM\n",
        "\n",
        "在ML cyber这门课上，教授给我们review了SVM。我们需要了解SVM是怎么work的，也需要知道其数学过程。在我们了解其过程之后，我们会做一个小lab来加深对其的理解。\n",
        "\n",
        "In Machine Learning Cyber class, we reviewed SVM. We need to know how does SVM work and understand its mathematical process also. After review them, we will do a small project to have a deep understanding \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq9ky35Fa4jR"
      },
      "source": [
        "## The intuitive understanding of SVM\n",
        "\n",
        "Now, we have two classes that can be linearly separated. Below figure shows the case we are assuming.\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/68700549/134811043-62c61f8e-38fd-492e-bff7-07e7d2a2c77b.png\" alt=\"WeChat Screenshot_20210926095925\" style=\"zoom:10%;\" />\n",
        "\n",
        "现在呢，我们可以看到有两个类，可以被线性分开，图中我们也可以看到有两条线都可以将其分开，但是哪一条更好呢？\n",
        "\n",
        "Now, we can that there are two lines can linearly separated the classess. But which one is better? \n",
        "\n",
        "\n",
        "![WeChat Screenshot_20210926100557](https://user-images.githubusercontent.com/68700549/134811213-072c5509-c815-489c-a52d-7efe36e72470.png)\n",
        "\n",
        "于是就有了SVM，我们希望能找到一条线，就是找到两个类的边界，然后我们找到的线可以使得到这两条线的距离相等。就如SVM的图所示.\n",
        "\n",
        "SVM, support vector machine, will help find a line that can linearly separated them well. The SVM will find the boundary of both classes and the line will be posited between them. We hope the distance between both classes' boundary points can be equal. Look at the graph above.\n",
        "\n",
        "假设能找到线，我们就希望能够最大化这个距离，让两个classes尽可能地分开。如果有新数据进来，那么我们找到的线也是能够尽可能地正确分类新数据。\n",
        "\n",
        "We try to maximize the distance of the margin to separate two classes as much as possible. If new data comes in, the line has much higher probability to classifier it well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hQUIt31e2eG"
      },
      "source": [
        "## The mathematic of SVM\n",
        "\n",
        "现在我们来算一下穿过support vector (两个classes的边界点)的两条线之间的距离是多少，从图中，我们可以看到有两条线，一个是$wx+b=1$, 另一条是$wx+b=-1$. 这里先解释下为什么是1和-1，即使是其他的数，我们也是可以变换到1和-1来的，就是做乘除法而已。\n",
        "\n",
        "Now, we are calculating the distance between two lines that pass throught the support vecotrs (show in the graph). We can see there are two dashed line, one is $wx+b=1$, another one is $wx+b=-1$. The reason why it is 1 and -1. It actually does not matter. We can transform it to 1 and -1 even though they are not. Just do some multiplication and division. Using 1 and -1 is much easier. \n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/68700549/134814170-94381088-5f22-4ba3-910b-92a523d453d8.png\" alt=\"WeChat Screenshot_20210926112656\" style=\"zoom:50%;\" />\n",
        "\n",
        "这两条线的距离其实就是$d_1+d_2$​​​. 我们需要知道点到线的距离的计算公式，如下\n",
        "\n",
        "The distance between these two lines is $d_1+d_2$. The way to calculate the distance from a point to a line is shown below.​\n",
        "$$\n",
        "r = \\frac{|wx+b|}{||w||_2}\n",
        "$$\n",
        "\n",
        "现在，我们要计算$d_1+d_2$​, 也就是要看support vector，那么也就是\n",
        "\n",
        "Now, we need to calculate the $d_1+d_2$. So, we need to look at the support vectors. Then we get the distance.​\n",
        "\n",
        "$$\n",
        "\\gamma = d_1+d_2=\\frac{|wx_1+b|+|wx_2+b|}{||w||_2}=\\frac{2}{||w||_2}\n",
        "$$\n",
        "\n",
        "然后，现在我们想要最大化这个距离.于是我们就要做一个约束优化. 上面的$\\frac{2}{||w||_2}$可以写成下面的形式. $y_i(w^Tx_i+b)\\ge 1$, 也就是分类正确的意思\n",
        "\n",
        "We want to maximize the distance. We can do a constraint optimization. $y_i(w^Tx_i+b)\\ge 1$ means classify points correctly.\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "min_{w,b}\\frac{1}{2}||w||^2\\\\\n",
        "s.t.  \\space y_i(w^Tx_i+b)\\ge 1, i=1,\\cdots, n\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "遇到约束优化，我们就可以进行拉格朗日化，但前提要变成对偶条件。也就是primal dual，对于原优化问题\n",
        "\n",
        "when we have constraint optimization, we can think about Lagrange multipliers. We should make the optimization become primal dual problem. We should make things like below\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "\\mathop{min}\\limits_{w} f(w)\\\\\n",
        "s.t. \\space\\space \\begin{matrix}\n",
        "g_i(w)\\le 0\\\\\n",
        "h_i(w)\\le 0\n",
        "\\end{matrix}\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "这种形式下，它的拉格朗日函数如下\n",
        "\n",
        "In this format, its Lagrange function is below\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(w,\\alpha,\\beta)=f(w)+\\sum_{i=1}^k \\alpha_i g_i(w)+\\sum_{i=1}^l \\beta_i h_i(w)\n",
        "$$\n",
        "\n",
        "此时，问题就变成了$\\mathop{max}\\limits_{\\alpha,\\beta:\\alpha\\ge0}\\mathcal{L}(w,\\alpha,\\beta)$​\n",
        "\n",
        "Now, the question becomes $\\mathop{max}\\limits_{\\alpha,\\beta:\\alpha\\ge0}\\mathcal{L}(w,\\alpha,\\beta)$\n",
        "\n",
        "同理，对于我们的SVM，也是相同的做法，我们先把原始优化变成primal dual的形式，也就是\n",
        "\n",
        "So, for SVM, we do the same thing. We need to make it prime dual first. It is like below.\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "min_{w,b}\\frac{1}{2}||w||^2\\\\\n",
        "s.t.  \\space 1-y_i(w^Tx_i+b)\\le 0, i=1,\\cdots, n\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "于是，它的拉格朗日函数变成了\n",
        "\n",
        "And its Lagrange function is below. \n",
        "\n",
        "$$\n",
        "\\mathcal{L}(w,b,\\alpha)=\\frac{1}{2}||w||^2 + \\sum_{i=1}^n \\alpha_i (1-y_i(w^Tx_i+b))\n",
        "$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkB8jGtZazTs"
      },
      "source": [
        "\n",
        "### 带松弛变量的SVM\n",
        "### 带kernel的SVM\n",
        "### SVM的smo解法\n",
        "### 使用SVM多类别分类"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6UyJh2PYjDL"
      },
      "source": [
        "# SVM人脸识别结合cross-validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnpVLVXxYrAG"
      },
      "source": [
        "# 模型评估方法和SVM做人脸识别"
      ]
    }
  ]
}