{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVM_lab.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNWr7AJYkilZKLOsTBFt4hI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeb2Binfang/ECE-GY-9163-ML-cyber/blob/main/Lab/SVM_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZiCfDOE3jsb"
      },
      "source": [
        "# Basic knowledge of SVM\n",
        "\n",
        "在ML cyber这门课上，教授给我们review了SVM。我们需要了解SVM是怎么work的，也需要知道其数学过程。在我们了解其过程之后，我们会做一个小lab来加深对其的理解。\n",
        "\n",
        "In Machine Learning Cyber class, we reviewed SVM. We need to know how does SVM work and understand its mathematical process also. After review them, we will do a small project to have a deep understanding \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq9ky35Fa4jR"
      },
      "source": [
        "## The intuitive understanding of SVM\n",
        "\n",
        "Now, we have two classes that can be linearly separated. Below figure shows the case we are assuming.\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/68700549/134811043-62c61f8e-38fd-492e-bff7-07e7d2a2c77b.png\" alt=\"WeChat Screenshot_20210926095925\" style=\"zoom:10%;\" />\n",
        "\n",
        "现在呢，我们可以看到有两个类，可以被线性分开，图中我们也可以看到有两条线都可以将其分开，但是哪一条更好呢？\n",
        "\n",
        "Now, we can that there are two lines can linearly separated the classess. But which one is better? \n",
        "\n",
        "\n",
        "![WeChat Screenshot_20210926100557](https://user-images.githubusercontent.com/68700549/134811213-072c5509-c815-489c-a52d-7efe36e72470.png)\n",
        "\n",
        "于是就有了SVM，我们希望能找到一条线，就是找到两个类的边界，然后我们找到的线可以使得到这两条线的距离相等。就如SVM的图所示.\n",
        "\n",
        "SVM, support vector machine, will help find a line that can linearly separated them well. The SVM will find the boundary of both classes and the line will be posited between them. We hope the distance between both classes' boundary points can be equal. Look at the graph above.\n",
        "\n",
        "假设能找到线，我们就希望能够最大化这个距离，让两个classes尽可能地分开。如果有新数据进来，那么我们找到的线也是能够尽可能地正确分类新数据。\n",
        "\n",
        "We try to maximize the distance of the margin to separate two classes as much as possible. If new data comes in, the line has much higher probability to classifier it well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hQUIt31e2eG"
      },
      "source": [
        "## The mathematic of SVM\n",
        "\n",
        "现在我们来算一下穿过support vector (两个classes的边界点)的两条线之间的距离是多少，从图中，我们可以看到有两条线，一个是$wx+b=1$, 另一条是$wx+b=-1$. 这里先解释下为什么是1和-1，即使是其他的数，我们也是可以变换到1和-1来的，就是做乘除法而已。\n",
        "\n",
        "Now, we are calculating the distance between two lines that pass throught the support vecotrs (show in the graph). We can see there are two dashed line, one is $wx+b=1$, another one is $wx+b=-1$. The reason why it is 1 and -1. It actually does not matter. We can transform it to 1 and -1 even though they are not. Just do some multiplication and division. Using 1 and -1 is much easier. \n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/68700549/134814170-94381088-5f22-4ba3-910b-92a523d453d8.png\" alt=\"WeChat Screenshot_20210926112656\" style=\"zoom:50%;\" />\n",
        "\n",
        "这两条线的距离其实就是$d_1+d_2$​​​. 我们需要知道点到线的距离的计算公式，如下\n",
        "\n",
        "The distance between these two lines is $d_1+d_2$. The way to calculate the distance from a point to a line is shown below.​\n",
        "$$\n",
        "r = \\frac{|wx+b|}{||w||_2}\n",
        "$$\n",
        "\n",
        "现在，我们要计算$d_1+d_2$​, 也就是要看support vector，那么也就是\n",
        "\n",
        "Now, we need to calculate the $d_1+d_2$. So, we need to look at the support vectors. Then we get the distance.​\n",
        "\n",
        "$$\n",
        "\\gamma = d_1+d_2=\\frac{|wx_1+b|+|wx_2+b|}{||w||_2}=\\frac{2}{||w||_2}\n",
        "$$\n",
        "\n",
        "然后，现在我们想要最大化这个距离.于是我们就要做一个约束优化. 上面的$\\frac{2}{||w||_2}$可以写成下面的形式. $y_i(w^Tx_i+b)\\ge 1$, 也就是分类正确的意思\n",
        "\n",
        "We want to maximize the distance. We can do a constraint optimization. $y_i(w^Tx_i+b)\\ge 1$ means classify points correctly.\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "min_{w,b}\\frac{1}{2}||w||^2\\\\\n",
        "s.t.  \\space y_i(w^Tx_i+b)\\ge 1, i=1,\\cdots, n\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "遇到约束优化，我们就可以进行拉格朗日化，但前提要变成对偶条件。也就是primal dual，对于原优化问题\n",
        "\n",
        "when we have constraint optimization, we can think about Lagrange multipliers. We should make the optimization become primal dual problem. We should make things like below\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "\\mathop{min}\\limits_{w} f(w)\\\\\n",
        "s.t. \\space\\space \n",
        "\\begin{matrix}\n",
        "g_i(w)\\le 0\\\\\n",
        "h_i(w)\\le 0\n",
        "\\end{matrix}\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "这种形式下，它的拉格朗日函数如下\n",
        "\n",
        "In this format, its Lagrange function is below\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(w,\\alpha,\\beta)=f(w)+\\sum_{i=1}^k \\alpha_i g_i(w)+\\sum_{i=1}^l \\beta_i h_i(w)\n",
        "$$\n",
        "\n",
        "此时，问题就变成了$\\mathop{max}\\limits_{\\alpha,\\beta:\\alpha\\ge0}\\mathcal{L}(w,\\alpha,\\beta)$​\n",
        "\n",
        "Now, the question becomes $\\mathop{max}\\limits_{\\alpha,\\beta:\\alpha\\ge0}\\mathcal{L}(w,\\alpha,\\beta)$\n",
        "\n",
        "同理，对于我们的SVM，也是相同的做法，我们先把原始优化变成primal dual的形式，也就是\n",
        "\n",
        "So, for SVM, we do the same thing. We need to make it prime dual first. It is like below.\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "min_{w,b}\\frac{1}{2}||w||^2\\\\\n",
        "s.t.  \\space 1-y_i(w^Tx_i+b)\\le 0, i=1,\\cdots, n\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "于是，它的拉格朗日函数变成了\n",
        "\n",
        "And its Lagrange function is below. \n",
        "\n",
        "$$\n",
        "\\mathcal{L}(w,b,\\alpha)=\\frac{1}{2}||w||^2 + \\sum_{i=1}^n \\alpha_i (1-y_i(w^Tx_i+b))\n",
        "$$\n",
        "\n",
        "此时，我们要对$w,b$​进行求导\n",
        "\n",
        "Then, we can get the partial derivative of $w,b$.\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "\\triangledown_w\\mathcal{L}(w,b,\\alpha)=w-\\sum_{i=1}^n \\alpha_i y_i x_i=0\\\\\n",
        "\\triangledown_b\\mathcal{L}(w,b,\\alpha)=-\\sum_{i=1}^n \\alpha_i y_i=0\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "于是，我们可以得到 $w=\\sum_{i=1}^n \\alpha_i y_i x_i$​ and $\\sum_{i=1}^n \\alpha_i y_i=0$​​. \n",
        "\n",
        "Now, we can get  $w=\\sum_{i=1}^n \\alpha_i y_i x_i$ and $\\sum_{i=1}^n \\alpha_i y_i=0$. \n",
        "\n",
        "然后我们再把$w$​的值给代回到拉格朗日函数中，可以得到\n",
        "\n",
        "And we put $w$ into the Lagrange function\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "\\mathcal{L}(w,b,\\alpha)=\\frac{1}{2}||\\sum_{i=1}^n \\alpha_i y_i x_i||^2+\\sum_{i=1}^n \\alpha_i(1-y_i((\\sum_{j=1}^n \\alpha_j y_j x_j)^Tx_i +b))\\\\\n",
        "=\\frac{1}{2}(\\sum_{i=1}^n \\alpha_i y_i x_i)^T (\\sum_{j=1}^n \\alpha_j y_j x_j)+\\sum_{i=1}^n \\alpha_i -\\sum_{i=1}^n \\alpha_i y_i(\\sum_{j=1}^n \\alpha_j y_j x_j)^Tx_i - \\sum_{i=1}^n \\alpha_i y_ib\\\\\n",
        "=\\frac{1}{2}\\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j x_i^T x_j+\\sum_{i=1}^n \\alpha_i -\\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j x_i^T x_j\\\\\n",
        "=\\sum_{i=1}^n \\alpha_i-\\frac{1}{2}\\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j x_i^T x_j\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "现在，我们函数中已经没有了$w,b$, 只剩下了$\\alpha$​. 所以，我们构造的对偶问题就是\n",
        "\n",
        "Now, in the function, we do not have $w,b$ and only $\\alpha$ left, so the dual problem we constructed is shown below\n",
        "\n",
        "$$\n",
        "\\begin{gathered}\n",
        "max_{\\alpha}= \\sum_{i=1}^n \\alpha_i-\\frac{1}{2}\\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j x_i^T x_j\\\\\n",
        "s.t. \\space \\space\n",
        "\\begin{matrix}\\alpha_i \\ge0\\\\\n",
        "\\sum_{i=1}^n \\alpha_i y_i=0\n",
        "\\end{matrix}\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "SVM的决策式子，也就是SVM的模型，就可以写成, 这里呢，每一个$x$，都会有一个对应的$\\alpha$​.\n",
        "\n",
        "SVM decision formula is shown below. We can know for every $x$, there is a corresponding $\\alpha$\n",
        "\n",
        "$$\n",
        "f(x)=w^Tx+b=\\sum_{i=1}^n \\alpha_i y_i x_i^T x +b\n",
        "$$\n",
        "\n",
        "这里，我们会有一个疑问，每次测试一个新样本，就需要跟每一个数据进行转置乘积并求和，哇，假设有一个亿的数据，那岂不是要有很大的空间来存储。看到这里，我们必须要清楚，我们的式子是有不等式约束的，也就是要满足KKT条件.\n",
        "\n",
        "Here, we may have a question. When we test a new test data, then we need to transpose every $x$​​ and get the cumulative sum. If we have a billion data, then we need a lot of space to store it. But, we need to know, our formula we got lastly satisfies the kKT conditions which is shown below\n",
        "\n",
        "$$\n",
        "\\begin{cases}\n",
        "\\alpha_i\\ge0 \\\\\n",
        "y_if(x_i)-1\\ge0\\\\\n",
        "\\alpha_i(y_if(x_i)-1)=0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "所以，我们是不用担心这么多数据的，根据KKT条件的第三行，可以得知非support vector的$\\alpha=0$. 只有support vector，$y_if(x_i)-1=0$​. 所以，这里就有一个SVM的重要性质，就是训练完成后，大部分的训练样本都不需要保留，最终模型只跟support vector相关。\n",
        "\n",
        "So, we do need to worry about it. According to the third line of KKT conditions, we can know that the $\\alpha$ value of non support vectors are all equal to 0. Therefore, there is a very important property of SVM that the final model only associates with support vectors and most of training data does not need to be stored.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkB8jGtZazTs"
      },
      "source": [
        "## SVM with slack variable\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ImzO22NCBOt"
      },
      "source": [
        "## 带kernel的SVM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3n6qRiuCcVA"
      },
      "source": [
        "## SVM的smo解法\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWfp3ITRCgdj"
      },
      "source": [
        "## 使用SVM多类别分类"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6UyJh2PYjDL"
      },
      "source": [
        "# SVM人脸识别结合cross-validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnpVLVXxYrAG"
      },
      "source": [
        "# 模型评估方法和SVM做人脸识别"
      ]
    }
  ]
}