{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPleCV9qQSfWeKsH6+SIfUL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeb2Binfang/ECE-GY-9163-ML-cyber/blob/main/Lab/MLSec_Lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V_0_tYWBYYu"
      },
      "source": [
        "# ECE-GY 9163 ML Cyber Lab1-Email spam filtering\n",
        "\n",
        "## Overview\n",
        "\n",
        "In this lab, you will design an e-mail spam filter using a Naive Bayes and SVM based classification on the ling-spam dataset. You will explore the impact of feature selection and compare the performance of different variants of an NB classifier and also implement your own SVM based classifier. (Note: You may use the scikitt learn classifiers to only compare the accuracy\n",
        "of their model to yours).\n",
        "\n",
        "## Dataset\n",
        "\n",
        "The ling-spam corpus contains e-mails from the Linguist mailing list categorized as either legitimate or spam emails. The corpus is divided into four sub-folders that contain the same emails that are pre-processed with/without lemmatization and with/without stop-word removal. The e-mails in each sub-folder partitioned into 10 \"folds.\"\n",
        "\n",
        "In this lab, we will use the first 9 folds from the ling-spam corpus as training data, and the $10^{th}$ fold as test data.\n",
        "\n",
        "## What you have to do\n",
        "\n",
        "You will implement your e-mail spam filters in Python. You are free to use any Python libraries that are relevant to the problem.\n",
        "\n",
        "* Download the ling-spam dataset from http://www.aueb.gr/users/ion/data/lingspam_public.tar.gz\n",
        "\n",
        "Please use the \"lingspam_public01' corpus with both lemmatization and stop-word enabled (under the lemm_stop folder).\n",
        "\n",
        "* Your first goal is to perform feature selection using the information gain (IG) metric. From the training data, select the top-N features for N = {10, 100, 1000}. Note that feature selection based on the IG metric only accounts for the occurrence of (and not frequency with which terms appear) in the dataset.\n",
        "\n",
        "* Next, implement the following classifiers:\n",
        "  * Bernoulli NB classifier with binary features;\n",
        "  * Multinomial NB with binary features, and\n",
        "  * Multinomial NB with term frequency (TF) features.\n",
        "\n",
        "* For each of the three classifiers above and for N = {10, 100, 1000} report the spam precision and spam recall. Spam precision is defined as the fraction of true spam e-mails among all e-mails predicted as spam, and spam recall is defined as fraction of true spam e-mails predicted as spam. Also report the latency of each model.\n",
        "* Design a Support Vector Machine (SVM) based spam filter. This problem is open ended: for instance, you can choose to use either BF or TF and any feature selection method. Note that you should NOT use the test dataset in picking the hyper-parameters of your spam filter, instead use cross-validation on the training dataset.\n",
        "\n",
        "Finally, we covered \"Adversarial Classification\" (https://dl.acm.org/citation.cfm?id=1014066), an approach to update NB based e-mail spam filters in response to attacks that try to evade a basic NB filter. You are expected to implement the techniques presented in this paper. More specifically, you can make the following assumptions:\n",
        "* The baseline NB classifier uses the **top-10** terms identified using the IG metric and using Boolean features.\n",
        "* The adversary uses the **ADD-WORDS** strategy. Adding a term to an email incurs unit cost. The attacker seeks to find the minimum cost solution such that each spam email in the test set gets classified as legitimate by the baseline NB classifier.\n",
        "* Update the baseline NB classifier in response to the attacker's strategy above. You can assume that the defender\n",
        "pays a unit price for both false positives and false negatives.\n",
        "\n",
        "## What to submit\n",
        "\n",
        "* Your Python code in the form of a Google Colab notebook. Please also include a PDF of your colab notebook.\n",
        "* Your Colab notebook should print:\n",
        "* a list of the top-10 words identified from Part (1) above, and\n",
        "* a list of spam precision and spam recall values for each of the three classifiers for N = {10, 100, 1000}. That is, your list should have 9 rows, one for each classifier and N combination.\n",
        "* For the SVM based spam filter, please describe your methodology, i.e., what kind of features you used, how many features you used and how you selected them, the parameters of the SVM and finally the precision and recall on\n",
        "the test dataset.\n",
        "* For Your Neural network model, please report on the model design, input parameters, number of layers, number\n",
        "of neurons in each layer and finally the accuracy and latency of the model.\n",
        "* For the adversarial attack, report the False Negative rate of the baseline NB classifier before and after the attacker's modifications to test emails. Also, report the average \"cost of the attacker's modifications, averaged\n",
        "over all spam emails in the test set. Finally, report the False Negative and False Positive rate of the updated NBclassifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nD8ZH3yoXWiX"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJqfltNlBXgk"
      },
      "source": [
        "import numpy as np\n",
        "import tarfile\n",
        "import os\n",
        "import string"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUJ11mx8Xbzk"
      },
      "source": [
        "## Load dataset and process data\n",
        "\n",
        "Here, we will use files that under lemm_stop folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLKCAze6Y-je",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdeeb751-5dd7-4b7c-8f6a-3b32c9aec694"
      },
      "source": [
        "!wget \"http://www.aueb.gr/users/ion/data/lingspam_public.tar.gz\" -O lingspam_public.tar.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-24 02:00:21--  http://www.aueb.gr/users/ion/data/lingspam_public.tar.gz\n",
            "Resolving www.aueb.gr (www.aueb.gr)... 195.251.255.156\n",
            "Connecting to www.aueb.gr (www.aueb.gr)|195.251.255.156|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://www2.aueb.gr/users/ion/data/lingspam_public.tar.gz [following]\n",
            "--2021-10-24 02:00:22--  http://www2.aueb.gr/users/ion/data/lingspam_public.tar.gz\n",
            "Resolving www2.aueb.gr (www2.aueb.gr)... 195.251.255.138\n",
            "Connecting to www2.aueb.gr (www2.aueb.gr)|195.251.255.138|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11564714 (11M) [application/x-gzip]\n",
            "Saving to: ‘lingspam_public.tar.gz’\n",
            "\n",
            "lingspam_public.tar 100%[===================>]  11.03M  1.19MB/s    in 17s     \n",
            "\n",
            "2021-10-24 02:00:41 (665 KB/s) - ‘lingspam_public.tar.gz’ saved [11564714/11564714]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeXjzVm4ahj1"
      },
      "source": [
        "!tar -xvzf \"lingspam_public.tar.gz\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmeySJYWpz6Q"
      },
      "source": [
        "Lemmatisation: Lemmatisation (or lemmatization) in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form. n many languages, words appear in several inflected forms. For example, in English, the verb 'to walk' may appear as 'walk', 'walked', 'walks' or 'walking'. The base form, 'walk', that one might look up in a dictionary, is called the lemma for the word. The association of the base form with a part of speech is often called a lexeme of the word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7iISbRgSYc6"
      },
      "source": [
        "We will use the lemm_stop dataset. There are 10 folders totally. We need to count how many spam emails and how many ham email. We will use it later to calculate the probability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT0Rh6w2TET7"
      },
      "source": [
        "lemm_stop = ['lingspam_public/lemm_stop/part1/', 'lingspam_public/lemm_stop/part2/', 'lingspam_public/lemm_stop/part3/',\n",
        "             'lingspam_public/lemm_stop/part4/', 'lingspam_public/lemm_stop/part5/', 'lingspam_public/lemm_stop/part6/',\n",
        "             'lingspam_public/lemm_stop/part7/', 'lingspam_public/lemm_stop/part8/', 'lingspam_public/lemm_stop/part9/',\n",
        "             'lingspam_public/lemm_stop/part10/']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjAG-5Onlkh0"
      },
      "source": [
        "processData function will process all txt data. The function will return a nested dict finally. It will remove all \"\\n\",\"\\t\" and all punctuation such as '!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~'. Finally, the txt data will become a list. For example: ['hello','world']. For each email, it will be put to a nest fict. Here, we have 10 folders totally, the final result for this function is \n",
        "\n",
        "{\n",
        "  \n",
        "\"lingspam_public/lemm_stop/part1\":\n",
        "\n",
        "{\n",
        "  \n",
        "\"3-565msg2.txt\":[],\n",
        "\n",
        "\"3-1msg1.txt\":[],...\n",
        "\n",
        "}\n",
        "\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYBPjxXRWpCN"
      },
      "source": [
        "def processData(dir_list):\n",
        "  all_data = {}\n",
        "  for dir in dir_list:\n",
        "     all_data[dir]={} # a nested dict\n",
        "     file_list = os.listdir(dir) # list all files in a dir   \n",
        "     for file_name in file_list: # go through all files   \n",
        "        f = open(dir+file_name, \"r\")\n",
        "        data = f.read()\n",
        "        data = data.lower() # make all letters to lowercase\n",
        "        data = data.replace('\\n','') # remove all next_line\n",
        "        data = data.replace('\\t','') # remove all tag\n",
        "        data = data.replace(\"subject\",'') # remove subject, it exists all emails, that is why we remove it.\n",
        "        punctuation_string = string.punctuation # remove all punctuation such as !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
        "        for i in punctuation_string:\n",
        "          data = data.replace(i,'')\n",
        "        data = data.split(' ')\n",
        "        data = ' '.join(data).split()\n",
        "        \n",
        "        all_data[dir][file_name] = data # add to dict\n",
        "        f.close()\n",
        "  return all_data"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmPO5cWjnkrR"
      },
      "source": [
        "all_data = processData(lemm_stop)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGPyrmVmiQ2g",
        "outputId": "b3ed8c92-065e-47a9-e8e1-a4b012447d89"
      },
      "source": [
        "## Show one example here to see what does data look like\n",
        "## print one list\n",
        "print(all_data['lingspam_public/lemm_stop/part2/']['5-1298msg3.txt'])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['job', 'postingannouncement', 'open', 'rank', 'professorial', 'position', 'university', 'california', 'san', 'diego', 'department', 'linguistics', 'availability', 'fund', 'department', 'lingui', 'tic', 'university', 'californium', 'san', 'diego', 'seek', 'fill', 'open', 'rank', 'professorial', 'position', 'tenure', 'tenure', 'track', 'effective', 'july', '1', '1995', 'linguist', 'capable', 'teach', 'formal', 'semantics', 'prove', 'research', 'record', 'formal', 'semantics', 'include', 'semantics', 'syntax', 'interface', 'salary', 'commensurate', 'rank', 'experience', 'base', 'current', 'university', 'californium', 'salary', 'scale', 'letter', 'application', 'curriculum', 'vita', 'representa', 'tive', 'publication', 'manuscript', 'name', 'address', '3', 'referee', 'send', 'university', 'californium', 'san', 'diego', 'open', 'search', 'committee', 'department', 'linguistic', '0108', '9500', 'gilman', 'drive', 'la', 'jollum', 'ca', '920930108', 'application', 'material', 'must', 'receive', 'later', 'febru', 'ary', '1', '1995', 'university', 'californium', 'equal', 'opportunity', 'affirmative', 'action', 'employer', 'announcement', 'supersede', 'our', 'october', 'lsa', 'bulletin', 'announcement', 'our', 'august', 'departmental', 'notice', 'tenure', 'position', 'formal', 'semantics', 'syntax']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0IX7R_lK1r9"
      },
      "source": [
        "We need to get the feature list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50jyJbbpK2zr"
      },
      "source": [
        "def getFeatureList(dir_list, all_data):\n",
        "  feature_list = []\n",
        "  for dir in dir_list:\n",
        "    file_list = os.listdir(dir) # list all files in a dir  \n",
        "    for file_name in file_list: # go through all files   \n",
        "      for item in all_data[dir][file_name]:\n",
        "        if item not in feature_list:\n",
        "          feature_list.append(item)\n",
        "  return feature_list"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JPxH74cQaKf"
      },
      "source": [
        "feature_list = getFeatureList(lemm_stop, all_data)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUqWJ6F9Qfhe",
        "outputId": "5ead802a-1801-4dc0-dc60-56edfe6f66c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"we have {} features in total.\".format(len(feature_list)))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we have 60854 features in total.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIDhlzcPqCj2"
      },
      "source": [
        "## Information gain calculation\n",
        "\n",
        "We can measure \"information\", it is a bernoulli process and the result has two variables $X\\in \\{0,1\\}$ where 0 is ham email and 1 is spam email. We assume that $P(X=1)=p$. We can calculate H(X). The formula is below\n",
        "$$\n",
        "H(X)=-plog(p)-(1-p)log(1-p)\n",
        "$$\n",
        "where $p$ is the prior probability of spam email. \n",
        "\n",
        "So we need to count how many spam and ham emails in the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z4uITZvqHMf"
      },
      "source": [
        "def countTheNumberOfHamAndSpam(dir_list):\n",
        "  count_spam = 0\n",
        "  count_ham = 0\n",
        "  for dir in dir_list:\n",
        "    file_list = os.listdir(dir) # list all files in a dir \n",
        "    for file_name in file_list: \n",
        "      if 'spm' in file_name:\n",
        "        count_spam+=1\n",
        "      else:\n",
        "        count_ham+=1\n",
        "  return count_spam, count_ham"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5gCANLSGYPl"
      },
      "source": [
        "count_spam, count_ham = countTheNumberOfHamAndSpam(lemm_stop)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-yo1CB3HT_1",
        "outputId": "7de41b0e-cae4-48fc-ed0a-81393f626ded",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "## print out the number of spam and ham emails\n",
        "print(\"We have {} spam emails, and {} ham emails totally\".format(count_spam, count_ham))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have 481 spam emails, and 2412 ham emails totally\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1hRV_ObJesV"
      },
      "source": [
        "Here, we can calculate H(X). We probably need to use Laplacian smoothing here because we need to calculate the probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5E2V6-NH9m8",
        "outputId": "2010c357-7e2e-4eb0-f4ae-970d5583b97f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "p_spam = (count_spam+1)/(count_spam+count_ham+2)\n",
        "H_p_spam = -p_spam*np.log(p_spam)-(1-p_spam)*np.log((1-p_spam))\n",
        "print(\"The H(X) is {}:\".format(H_p_spam))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The H(X) is 0.45028313289981026:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fVY3hM3U4-E"
      },
      "source": [
        "For a specific word, we can calculate the information gain too. The formula is \n",
        "$$\n",
        "IG(C,X_i)=H(C)-H(C|X_i)\n",
        "$$\n",
        "\n",
        "We have calculated $H(C)$ above, which is H(X).\n",
        "\n",
        "Here, we need to calculate $H(C|X_i)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QkO8JDjKSIj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}